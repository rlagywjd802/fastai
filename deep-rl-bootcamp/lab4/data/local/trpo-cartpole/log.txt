[2017-11-04 07:11:39.537154 UTC] Starting env pool
[2017-11-04 07:11:39.645098 UTC] Starting iteration 0
[2017-11-04 07:11:39.645831 UTC] Start collecting samples
[2017-11-04 07:11:39.904367 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:39.950481 UTC] Performing policy update
[2017-11-04 07:11:39.951079 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:39.963255 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:40.063993 UTC] Performing line search
[2017-11-04 07:11:40.069320 UTC] Updating baseline
[2017-11-04 07:11:40.182212 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| ExpectedImprovement  | 0.032059   |
| ActualImprovement    | 0.01459    |
| ImprovementRatio     | 0.4551     |
| MeanKL               | 0.0042504  |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2017-11-04 07:11:40.225563 UTC] Saving snapshot
[2017-11-04 07:11:40.230729 UTC] Starting iteration 1
[2017-11-04 07:11:40.230933 UTC] Start collecting samples
[2017-11-04 07:11:40.461451 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:40.504722 UTC] Performing policy update
[2017-11-04 07:11:40.505362 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:40.513617 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:40.606669 UTC] Performing line search
[2017-11-04 07:11:40.612432 UTC] Updating baseline
[2017-11-04 07:11:40.724233 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| ExpectedImprovement  | 0.037059  |
| ActualImprovement    | 0.032821  |
| ImprovementRatio     | 0.88565   |
| MeanKL               | 0.0088906 |
| Entropy              | 0.68726   |
| Perplexity           | 1.9883    |
| AveragePolicyProb[0] | 0.51726   |
| AveragePolicyProb[1] | 0.48274   |
| AverageReturn        | 25.67     |
| MinReturn            | 9         |
| MaxReturn            | 76        |
| StdReturn            | 14.792    |
| AverageEpisodeLength | 25.67     |
| MinEpisodeLength     | 9         |
| MaxEpisodeLength     | 76        |
| StdEpisodeLength     | 14.792    |
| TotalNEpisodes       | 146       |
| TotalNSamples        | 3684      |
| ExplainedVariance    | 0.21373   |
------------------------------------
[2017-11-04 07:11:40.768028 UTC] Saving snapshot
[2017-11-04 07:11:40.772728 UTC] Starting iteration 2
[2017-11-04 07:11:40.772932 UTC] Start collecting samples
[2017-11-04 07:11:40.995225 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:41.026476 UTC] Performing policy update
[2017-11-04 07:11:41.027087 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:41.034939 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:41.127509 UTC] Performing line search
[2017-11-04 07:11:41.137161 UTC] Updating baseline
[2017-11-04 07:11:41.240281 UTC] Computing logging information
-----------------------------------
| Iteration            | 2        |
| ExpectedImprovement  | 0.03247  |
| ActualImprovement    | 0.030734 |
| ImprovementRatio     | 0.94654  |
| MeanKL               | 0.006473 |
| Entropy              | 0.66993  |
| Perplexity           | 1.9541   |
| AveragePolicyProb[0] | 0.50524  |
| AveragePolicyProb[1] | 0.49476  |
| AverageReturn        | 37       |
| MinReturn            | 9        |
| MaxReturn            | 155      |
| StdReturn            | 26.351   |
| AverageEpisodeLength | 37       |
| MinEpisodeLength     | 9        |
| MaxEpisodeLength     | 155      |
| StdEpisodeLength     | 26.351   |
| TotalNEpisodes       | 194      |
| TotalNSamples        | 5820     |
| ExplainedVariance    | 0.13918  |
-----------------------------------
[2017-11-04 07:11:41.281120 UTC] Saving snapshot
[2017-11-04 07:11:41.285684 UTC] Starting iteration 3
[2017-11-04 07:11:41.285877 UTC] Start collecting samples
[2017-11-04 07:11:41.475497 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:41.497156 UTC] Performing policy update
[2017-11-04 07:11:41.497957 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:41.506260 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:41.602172 UTC] Performing line search
[2017-11-04 07:11:41.607585 UTC] Updating baseline
[2017-11-04 07:11:41.715155 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| ExpectedImprovement  | 0.037409  |
| ActualImprovement    | 0.02592   |
| ImprovementRatio     | 0.69289   |
| MeanKL               | 0.0069128 |
| Entropy              | 0.64837   |
| Perplexity           | 1.9124    |
| AveragePolicyProb[0] | 0.5244    |
| AveragePolicyProb[1] | 0.4756    |
| AverageReturn        | 40.84     |
| MinReturn            | 9         |
| MaxReturn            | 167       |
| StdReturn            | 30.612    |
| AverageEpisodeLength | 40.84     |
| MinEpisodeLength     | 9         |
| MaxEpisodeLength     | 167       |
| StdEpisodeLength     | 30.612    |
| TotalNEpisodes       | 216       |
| TotalNSamples        | 6906      |
| ExplainedVariance    | 0.1666    |
------------------------------------
[2017-11-04 07:11:41.758364 UTC] Saving snapshot
[2017-11-04 07:11:41.763008 UTC] Starting iteration 4
[2017-11-04 07:11:41.763315 UTC] Start collecting samples
[2017-11-04 07:11:42.011865 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:42.042695 UTC] Performing policy update
[2017-11-04 07:11:42.043332 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:42.053826 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:42.173884 UTC] Performing line search
[2017-11-04 07:11:42.185216 UTC] Updating baseline
[2017-11-04 07:11:42.328919 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| ExpectedImprovement  | 0.037922  |
| ActualImprovement    | 0.023631  |
| ImprovementRatio     | 0.62317   |
| MeanKL               | 0.0062252 |
| Entropy              | 0.62862   |
| Perplexity           | 1.875     |
| AveragePolicyProb[0] | 0.51763   |
| AveragePolicyProb[1] | 0.48237   |
| AverageReturn        | 56.95     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 46.119    |
| AverageEpisodeLength | 56.95     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 46.119    |
| TotalNEpisodes       | 241       |
| TotalNSamples        | 9276      |
| ExplainedVariance    | 0.26829   |
------------------------------------
[2017-11-04 07:11:42.380397 UTC] Saving snapshot
[2017-11-04 07:11:42.387054 UTC] Starting iteration 5
[2017-11-04 07:11:42.387330 UTC] Start collecting samples
[2017-11-04 07:11:42.623662 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:42.646543 UTC] Performing policy update
[2017-11-04 07:11:42.647195 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:42.658025 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:42.767107 UTC] Performing line search
[2017-11-04 07:11:42.773552 UTC] Updating baseline
[2017-11-04 07:11:42.891785 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| ExpectedImprovement  | 0.03686   |
| ActualImprovement    | 0.022615  |
| ImprovementRatio     | 0.61352   |
| MeanKL               | 0.0058138 |
| Entropy              | 0.60791   |
| Perplexity           | 1.8366    |
| AveragePolicyProb[0] | 0.49436   |
| AveragePolicyProb[1] | 0.50564   |
| AverageReturn        | 65.14     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 50.712    |
| AverageEpisodeLength | 65.14     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 50.712    |
| TotalNEpisodes       | 252       |
| TotalNSamples        | 10398     |
| ExplainedVariance    | 0.53511   |
------------------------------------
[2017-11-04 07:11:42.949501 UTC] Saving snapshot
[2017-11-04 07:11:42.954567 UTC] Starting iteration 6
[2017-11-04 07:11:42.954860 UTC] Start collecting samples
[2017-11-04 07:11:43.150550 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:43.167262 UTC] Performing policy update
[2017-11-04 07:11:43.167854 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:43.175677 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:43.277726 UTC] Performing line search
[2017-11-04 07:11:43.283498 UTC] Updating baseline
[2017-11-04 07:11:43.391119 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| ExpectedImprovement  | 0.033593  |
| ActualImprovement    | 0.019989  |
| ImprovementRatio     | 0.59502   |
| MeanKL               | 0.0096825 |
| Entropy              | 0.60011   |
| Perplexity           | 1.8223    |
| AveragePolicyProb[0] | 0.5279    |
| AveragePolicyProb[1] | 0.4721    |
| AverageReturn        | 81.37     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 59.799    |
| AverageEpisodeLength | 81.37     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.799    |
| TotalNEpisodes       | 266       |
| TotalNSamples        | 12550     |
| ExplainedVariance    | 0.60922   |
------------------------------------
[2017-11-04 07:11:43.434197 UTC] Saving snapshot
[2017-11-04 07:11:43.438572 UTC] Starting iteration 7
[2017-11-04 07:11:43.438767 UTC] Start collecting samples
[2017-11-04 07:11:43.688147 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:43.707213 UTC] Performing policy update
[2017-11-04 07:11:43.707961 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:43.717341 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:43.813347 UTC] Performing line search
[2017-11-04 07:11:43.819147 UTC] Updating baseline
[2017-11-04 07:11:43.924065 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| ExpectedImprovement  | 0.024377  |
| ActualImprovement    | 0.015851  |
| ImprovementRatio     | 0.65026   |
| MeanKL               | 0.0086385 |
| Entropy              | 0.58717   |
| Perplexity           | 1.7989    |
| AveragePolicyProb[0] | 0.513     |
| AveragePolicyProb[1] | 0.487     |
| AverageReturn        | 97.43     |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 64.94     |
| AverageEpisodeLength | 97.43     |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 64.94     |
| TotalNEpisodes       | 280       |
| TotalNSamples        | 14872     |
| ExplainedVariance    | 0.58924   |
------------------------------------
[2017-11-04 07:11:43.995734 UTC] Saving snapshot
[2017-11-04 07:11:44.004131 UTC] Starting iteration 8
[2017-11-04 07:11:44.004453 UTC] Start collecting samples
[2017-11-04 07:11:44.717702 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:44.761055 UTC] Performing policy update
[2017-11-04 07:11:44.762077 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:44.778007 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:44.957161 UTC] Performing line search
[2017-11-04 07:11:44.973789 UTC] Updating baseline
[2017-11-04 07:11:45.199842 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| ExpectedImprovement  | 0.026731  |
| ActualImprovement    | 0.0062879 |
| ImprovementRatio     | 0.23523   |
| MeanKL               | 0.0033178 |
| Entropy              | 0.58514   |
| Perplexity           | 1.7952    |
| AveragePolicyProb[0] | 0.54731   |
| AveragePolicyProb[1] | 0.45269   |
| AverageReturn        | 106.91    |
| MinReturn            | 11        |
| MaxReturn            | 200       |
| StdReturn            | 64.045    |
| AverageEpisodeLength | 106.91    |
| MinEpisodeLength     | 11        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 64.045    |
| TotalNEpisodes       | 291       |
| TotalNSamples        | 16458     |
| ExplainedVariance    | 0.59006   |
------------------------------------
[2017-11-04 07:11:45.284142 UTC] Saving snapshot
[2017-11-04 07:11:45.293793 UTC] Starting iteration 9
[2017-11-04 07:11:45.294445 UTC] Start collecting samples
[2017-11-04 07:11:45.808663 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:45.831950 UTC] Performing policy update
[2017-11-04 07:11:45.832743 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:45.842533 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:45.969246 UTC] Performing line search
[2017-11-04 07:11:45.975623 UTC] Updating baseline
[2017-11-04 07:11:46.123997 UTC] Computing logging information
------------------------------------
| Iteration            | 9         |
| ExpectedImprovement  | 0.038539  |
| ActualImprovement    | 0.019941  |
| ImprovementRatio     | 0.51741   |
| MeanKL               | 0.0052169 |
| Entropy              | 0.57752   |
| Perplexity           | 1.7816    |
| AveragePolicyProb[0] | 0.51207   |
| AveragePolicyProb[1] | 0.48793   |
| AverageReturn        | 124.11    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 60.581    |
| AverageEpisodeLength | 124.11    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 60.581    |
| TotalNEpisodes       | 307       |
| TotalNSamples        | 18721     |
| ExplainedVariance    | 0.39684   |
------------------------------------
[2017-11-04 07:11:46.184454 UTC] Saving snapshot
[2017-11-04 07:11:46.190924 UTC] Starting iteration 10
[2017-11-04 07:11:46.191417 UTC] Start collecting samples
[2017-11-04 07:11:46.418809 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:46.435305 UTC] Performing policy update
[2017-11-04 07:11:46.435847 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:46.444828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:46.536235 UTC] Performing line search
[2017-11-04 07:11:46.545219 UTC] Updating baseline
[2017-11-04 07:11:46.640971 UTC] Computing logging information
------------------------------------
| Iteration            | 10        |
| ExpectedImprovement  | 0.021923  |
| ActualImprovement    | 0.017333  |
| ImprovementRatio     | 0.79062   |
| MeanKL               | 0.0076674 |
| Entropy              | 0.57553   |
| Perplexity           | 1.7781    |
| AveragePolicyProb[0] | 0.52213   |
| AveragePolicyProb[1] | 0.47787   |
| AverageReturn        | 134.34    |
| MinReturn            | 13        |
| MaxReturn            | 200       |
| StdReturn            | 58.667    |
| AverageEpisodeLength | 134.34    |
| MinEpisodeLength     | 13        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 58.667    |
| TotalNEpisodes       | 316       |
| TotalNSamples        | 20340     |
| ExplainedVariance    | 0.41679   |
------------------------------------
[2017-11-04 07:11:46.685306 UTC] Saving snapshot
[2017-11-04 07:11:46.690165 UTC] Starting iteration 11
[2017-11-04 07:11:46.690352 UTC] Start collecting samples
[2017-11-04 07:11:46.908650 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:46.933460 UTC] Performing policy update
[2017-11-04 07:11:46.934113 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:46.944076 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:47.063238 UTC] Performing line search
[2017-11-04 07:11:47.069012 UTC] Updating baseline
[2017-11-04 07:11:47.172339 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| ExpectedImprovement  | 0.029019  |
| ActualImprovement    | 0.012393  |
| ImprovementRatio     | 0.42707   |
| MeanKL               | 0.0053151 |
| Entropy              | 0.56612   |
| Perplexity           | 1.7614    |
| AveragePolicyProb[0] | 0.52699   |
| AveragePolicyProb[1] | 0.47301   |
| AverageReturn        | 146.93    |
| MinReturn            | 22        |
| MaxReturn            | 200       |
| StdReturn            | 56.678    |
| AverageEpisodeLength | 146.93    |
| MinEpisodeLength     | 22        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 56.678    |
| TotalNEpisodes       | 329       |
| TotalNSamples        | 22748     |
| ExplainedVariance    | 0.57642   |
------------------------------------
[2017-11-04 07:11:47.220038 UTC] Saving snapshot
[2017-11-04 07:11:47.224696 UTC] Starting iteration 12
[2017-11-04 07:11:47.224922 UTC] Start collecting samples
[2017-11-04 07:11:47.412520 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:47.427315 UTC] Performing policy update
[2017-11-04 07:11:47.427966 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:47.435819 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:47.527370 UTC] Performing line search
[2017-11-04 07:11:47.532282 UTC] Updating baseline
[2017-11-04 07:11:47.633363 UTC] Computing logging information
-----------------------------------
| Iteration            | 12       |
| ExpectedImprovement  | 0.028562 |
| ActualImprovement    | 0.021787 |
| ImprovementRatio     | 0.76279  |
| MeanKL               | 0.007753 |
| Entropy              | 0.56016  |
| Perplexity           | 1.751    |
| AveragePolicyProb[0] | 0.5233   |
| AveragePolicyProb[1] | 0.4767   |
| AverageReturn        | 153.37   |
| MinReturn            | 22       |
| MaxReturn            | 200      |
| StdReturn            | 53.579   |
| AverageEpisodeLength | 153.37   |
| MinEpisodeLength     | 22       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 53.579   |
| TotalNEpisodes       | 339      |
| TotalNSamples        | 24400    |
| ExplainedVariance    | 0.55994  |
-----------------------------------
[2017-11-04 07:11:47.675906 UTC] Saving snapshot
[2017-11-04 07:11:47.680507 UTC] Starting iteration 13
[2017-11-04 07:11:47.680710 UTC] Start collecting samples
[2017-11-04 07:11:47.866874 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:47.881716 UTC] Performing policy update
[2017-11-04 07:11:47.882562 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:47.890535 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:47.981650 UTC] Performing line search
[2017-11-04 07:11:47.987602 UTC] Updating baseline
[2017-11-04 07:11:48.085860 UTC] Computing logging information
------------------------------------
| Iteration            | 13        |
| ExpectedImprovement  | 0.018492  |
| ActualImprovement    | 0.0063271 |
| ImprovementRatio     | 0.34216   |
| MeanKL               | 0.0059389 |
| Entropy              | 0.56644   |
| Perplexity           | 1.762     |
| AveragePolicyProb[0] | 0.50578   |
| AveragePolicyProb[1] | 0.49422   |
| AverageReturn        | 163.29    |
| MinReturn            | 28        |
| MaxReturn            | 200       |
| StdReturn            | 48.952    |
| AverageEpisodeLength | 163.29    |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 48.952    |
| TotalNEpisodes       | 349       |
| TotalNSamples        | 26396     |
| ExplainedVariance    | 0.75982   |
------------------------------------
[2017-11-04 07:11:48.132139 UTC] Saving snapshot
[2017-11-04 07:11:48.136787 UTC] Starting iteration 14
[2017-11-04 07:11:48.136994 UTC] Start collecting samples
[2017-11-04 07:11:48.329351 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:48.346076 UTC] Performing policy update
[2017-11-04 07:11:48.346708 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:48.354261 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:48.438609 UTC] Performing line search
[2017-11-04 07:11:48.443268 UTC] Updating baseline
[2017-11-04 07:11:48.542551 UTC] Computing logging information
------------------------------------
| Iteration            | 14        |
| ExpectedImprovement  | 0.017163  |
| ActualImprovement    | 0.0077693 |
| ImprovementRatio     | 0.45267   |
| MeanKL               | 0.0056084 |
| Entropy              | 0.56007   |
| Perplexity           | 1.7508    |
| AveragePolicyProb[0] | 0.50964   |
| AveragePolicyProb[1] | 0.49036   |
| AverageReturn        | 169.15    |
| MinReturn            | 28        |
| MaxReturn            | 200       |
| StdReturn            | 46.196    |
| AverageEpisodeLength | 169.15    |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 46.196    |
| TotalNEpisodes       | 358       |
| TotalNSamples        | 28196     |
| ExplainedVariance    | 0.65478   |
------------------------------------
[2017-11-04 07:11:48.586791 UTC] Saving snapshot
[2017-11-04 07:11:48.591671 UTC] Starting iteration 15
[2017-11-04 07:11:48.591878 UTC] Start collecting samples
[2017-11-04 07:11:48.810321 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:48.832663 UTC] Performing policy update
[2017-11-04 07:11:48.833308 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:48.844173 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:48.965418 UTC] Performing line search
[2017-11-04 07:11:48.971943 UTC] Updating baseline
[2017-11-04 07:11:49.098732 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| ExpectedImprovement  | 0.015289  |
| ActualImprovement    | 0.010025  |
| ImprovementRatio     | 0.65569   |
| MeanKL               | 0.0070706 |
| Entropy              | 0.57075   |
| Perplexity           | 1.7696    |
| AveragePolicyProb[0] | 0.51266   |
| AveragePolicyProb[1] | 0.48734   |
| AverageReturn        | 173.41    |
| MinReturn            | 28        |
| MaxReturn            | 200       |
| StdReturn            | 44.309    |
| AverageEpisodeLength | 173.41    |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 44.309    |
| TotalNEpisodes       | 368       |
| TotalNSamples        | 30196     |
| ExplainedVariance    | 0.76182   |
------------------------------------
[2017-11-04 07:11:49.153364 UTC] Saving snapshot
[2017-11-04 07:11:49.160178 UTC] Starting iteration 16
[2017-11-04 07:11:49.160423 UTC] Start collecting samples
[2017-11-04 07:11:49.357884 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:49.375227 UTC] Performing policy update
[2017-11-04 07:11:49.375889 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:49.385013 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:49.475820 UTC] Performing line search
[2017-11-04 07:11:49.480855 UTC] Updating baseline
[2017-11-04 07:11:49.575860 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| ExpectedImprovement  | 0.01925   |
| ActualImprovement    | 0.01009   |
| ImprovementRatio     | 0.52417   |
| MeanKL               | 0.0048469 |
| Entropy              | 0.55604   |
| Perplexity           | 1.7438    |
| AveragePolicyProb[0] | 0.50654   |
| AveragePolicyProb[1] | 0.49346   |
| AverageReturn        | 177.24    |
| MinReturn            | 28        |
| MaxReturn            | 200       |
| StdReturn            | 43.52     |
| AverageEpisodeLength | 177.24    |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 43.52     |
| TotalNEpisodes       | 380       |
| TotalNSamples        | 32596     |
| ExplainedVariance    | 0.69098   |
------------------------------------
[2017-11-04 07:11:49.627950 UTC] Saving snapshot
[2017-11-04 07:11:49.632540 UTC] Starting iteration 17
[2017-11-04 07:11:49.632852 UTC] Start collecting samples
[2017-11-04 07:11:49.818295 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:49.833570 UTC] Performing policy update
[2017-11-04 07:11:49.834152 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:49.841572 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:49.935014 UTC] Performing line search
[2017-11-04 07:11:49.943336 UTC] Updating baseline
[2017-11-04 07:11:50.041760 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| ExpectedImprovement  | 0.02263   |
| ActualImprovement    | 0.015138  |
| ImprovementRatio     | 0.66891   |
| MeanKL               | 0.0074632 |
| Entropy              | 0.56401   |
| Perplexity           | 1.7577    |
| AveragePolicyProb[0] | 0.52311   |
| AveragePolicyProb[1] | 0.47689   |
| AverageReturn        | 182.22    |
| MinReturn            | 28        |
| MaxReturn            | 200       |
| StdReturn            | 41.334    |
| AverageEpisodeLength | 182.22    |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 41.334    |
| TotalNEpisodes       | 390       |
| TotalNSamples        | 34568     |
| ExplainedVariance    | 0.73994   |
------------------------------------
[2017-11-04 07:11:50.089468 UTC] Saving snapshot
[2017-11-04 07:11:50.094452 UTC] Starting iteration 18
[2017-11-04 07:11:50.094683 UTC] Start collecting samples
[2017-11-04 07:11:50.304177 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:50.323743 UTC] Performing policy update
[2017-11-04 07:11:50.324325 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:50.331739 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:50.413707 UTC] Performing line search
[2017-11-04 07:11:50.418502 UTC] Updating baseline
[2017-11-04 07:11:50.511539 UTC] Computing logging information
-----------------------------------
| Iteration            | 18       |
| ExpectedImprovement  | 0.036031 |
| ActualImprovement    | 0.031492 |
| ImprovementRatio     | 0.87402  |
| MeanKL               | 0.00959  |
| Entropy              | 0.5631   |
| Perplexity           | 1.7561   |
| AveragePolicyProb[0] | 0.52658  |
| AveragePolicyProb[1] | 0.47342  |
| AverageReturn        | 180.8    |
| MinReturn            | 28       |
| MaxReturn            | 200      |
| StdReturn            | 45.385   |
| AverageEpisodeLength | 180.8    |
| MinEpisodeLength     | 28       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 45.385   |
| TotalNEpisodes       | 407      |
| TotalNSamples        | 36801    |
| ExplainedVariance    | 0.34755  |
-----------------------------------
[2017-11-04 07:11:50.563355 UTC] Saving snapshot
[2017-11-04 07:11:50.568151 UTC] Starting iteration 19
[2017-11-04 07:11:50.568354 UTC] Start collecting samples
[2017-11-04 07:11:50.777518 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:50.794248 UTC] Performing policy update
[2017-11-04 07:11:50.794896 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:50.803885 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:50.882029 UTC] Performing line search
[2017-11-04 07:11:50.887771 UTC] Updating baseline
[2017-11-04 07:11:50.983597 UTC] Computing logging information
------------------------------------
| Iteration            | 19        |
| ExpectedImprovement  | 0.023073  |
| ActualImprovement    | 0.01433   |
| ImprovementRatio     | 0.62109   |
| MeanKL               | 0.0081411 |
| Entropy              | 0.56094   |
| Perplexity           | 1.7523    |
| AveragePolicyProb[0] | 0.51396   |
| AveragePolicyProb[1] | 0.48604   |
| AverageReturn        | 181.42    |
| MinReturn            | 28        |
| MaxReturn            | 200       |
| StdReturn            | 45.405    |
| AverageEpisodeLength | 181.42    |
| MinEpisodeLength     | 28        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 45.405    |
| TotalNEpisodes       | 416       |
| TotalNSamples        | 38482     |
| ExplainedVariance    | 0.37498   |
------------------------------------
[2017-11-04 07:11:51.029426 UTC] Saving snapshot
[2017-11-04 07:11:51.034195 UTC] Starting iteration 20
[2017-11-04 07:11:51.034389 UTC] Start collecting samples
[2017-11-04 07:11:51.239624 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:51.256693 UTC] Performing policy update
[2017-11-04 07:11:51.257281 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:51.264294 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:51.356248 UTC] Performing line search
[2017-11-04 07:11:51.362085 UTC] Updating baseline
[2017-11-04 07:11:51.460009 UTC] Computing logging information
-----------------------------------
| Iteration            | 20       |
| ExpectedImprovement  | 0.016083 |
| ActualImprovement    | 0.014737 |
| ImprovementRatio     | 0.91635  |
| MeanKL               | 0.009073 |
| Entropy              | 0.55659  |
| Perplexity           | 1.7447   |
| AveragePolicyProb[0] | 0.50388  |
| AveragePolicyProb[1] | 0.49612  |
| AverageReturn        | 183.34   |
| MinReturn            | 28       |
| MaxReturn            | 200      |
| StdReturn            | 44.043   |
| AverageEpisodeLength | 183.34   |
| MinEpisodeLength     | 28       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 44.043   |
| TotalNEpisodes       | 426      |
| TotalNSamples        | 40482    |
| ExplainedVariance    | 0.25497  |
-----------------------------------
[2017-11-04 07:11:51.506866 UTC] Saving snapshot
[2017-11-04 07:11:51.511669 UTC] Starting iteration 21
[2017-11-04 07:11:51.511853 UTC] Start collecting samples
[2017-11-04 07:11:51.703873 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:51.720138 UTC] Performing policy update
[2017-11-04 07:11:51.720709 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:51.728118 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:51.819920 UTC] Performing line search
[2017-11-04 07:11:51.825014 UTC] Updating baseline
[2017-11-04 07:11:51.935705 UTC] Computing logging information
------------------------------------
| Iteration            | 21        |
| ExpectedImprovement  | 0.026278  |
| ActualImprovement    | 0.014245  |
| ImprovementRatio     | 0.54208   |
| MeanKL               | 0.0073549 |
| Entropy              | 0.57076   |
| Perplexity           | 1.7696    |
| AveragePolicyProb[0] | 0.50011   |
| AveragePolicyProb[1] | 0.49989   |
| AverageReturn        | 185.06    |
| MinReturn            | 31        |
| MaxReturn            | 200       |
| StdReturn            | 40.631    |
| AverageEpisodeLength | 185.06    |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 40.631    |
| TotalNEpisodes       | 436       |
| TotalNSamples        | 42306     |
| ExplainedVariance    | 0.24154   |
------------------------------------
[2017-11-04 07:11:51.996222 UTC] Saving snapshot
[2017-11-04 07:11:52.001226 UTC] Starting iteration 22
[2017-11-04 07:11:52.001428 UTC] Start collecting samples
[2017-11-04 07:11:52.183677 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:52.199280 UTC] Performing policy update
[2017-11-04 07:11:52.199848 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:52.208052 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:52.302571 UTC] Performing line search
[2017-11-04 07:11:52.307831 UTC] Updating baseline
[2017-11-04 07:11:52.411554 UTC] Computing logging information
------------------------------------
| Iteration            | 22        |
| ExpectedImprovement  | 0.02287   |
| ActualImprovement    | 0.012329  |
| ImprovementRatio     | 0.53908   |
| MeanKL               | 0.0067417 |
| Entropy              | 0.571     |
| Perplexity           | 1.77      |
| AveragePolicyProb[0] | 0.50045   |
| AveragePolicyProb[1] | 0.49955   |
| AverageReturn        | 185.1     |
| MinReturn            | 31        |
| MaxReturn            | 200       |
| StdReturn            | 40.644    |
| AverageEpisodeLength | 185.1     |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 40.644    |
| TotalNEpisodes       | 447       |
| TotalNSamples        | 44506     |
| ExplainedVariance    | 0.16129   |
------------------------------------
[2017-11-04 07:11:52.462543 UTC] Saving snapshot
[2017-11-04 07:11:52.468482 UTC] Starting iteration 23
[2017-11-04 07:11:52.468932 UTC] Start collecting samples
[2017-11-04 07:11:52.747627 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:52.769195 UTC] Performing policy update
[2017-11-04 07:11:52.769792 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:52.779824 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:52.897661 UTC] Performing line search
[2017-11-04 07:11:52.904289 UTC] Updating baseline
[2017-11-04 07:11:53.038687 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| ExpectedImprovement  | 0.014042  |
| ActualImprovement    | 0.0083936 |
| ImprovementRatio     | 0.59774   |
| MeanKL               | 0.0096384 |
| Entropy              | 0.56953   |
| Perplexity           | 1.7674    |
| AveragePolicyProb[0] | 0.50065   |
| AveragePolicyProb[1] | 0.49935   |
| AverageReturn        | 185.1     |
| MinReturn            | 31        |
| MaxReturn            | 200       |
| StdReturn            | 40.644    |
| AverageEpisodeLength | 185.1     |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 40.644    |
| TotalNEpisodes       | 456       |
| TotalNSamples        | 46306     |
| ExplainedVariance    | 0.26114   |
------------------------------------
[2017-11-04 07:11:53.092316 UTC] Saving snapshot
[2017-11-04 07:11:53.097156 UTC] Starting iteration 24
[2017-11-04 07:11:53.097343 UTC] Start collecting samples
[2017-11-04 07:11:53.285741 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:53.301100 UTC] Performing policy update
[2017-11-04 07:11:53.301753 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:53.309230 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:53.403458 UTC] Performing line search
[2017-11-04 07:11:53.408794 UTC] Updating baseline
[2017-11-04 07:11:53.513339 UTC] Computing logging information
------------------------------------
| Iteration            | 24        |
| ExpectedImprovement  | 0.016169  |
| ActualImprovement    | 0.010499  |
| ImprovementRatio     | 0.64932   |
| MeanKL               | 0.0097187 |
| Entropy              | 0.56419   |
| Perplexity           | 1.758     |
| AveragePolicyProb[0] | 0.49551   |
| AveragePolicyProb[1] | 0.50449   |
| AverageReturn        | 185.1     |
| MinReturn            | 31        |
| MaxReturn            | 200       |
| StdReturn            | 40.644    |
| AverageEpisodeLength | 185.1     |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 40.644    |
| TotalNEpisodes       | 467       |
| TotalNSamples        | 48506     |
| ExplainedVariance    | 0.0093125 |
------------------------------------
[2017-11-04 07:11:53.571080 UTC] Saving snapshot
[2017-11-04 07:11:53.576131 UTC] Starting iteration 25
[2017-11-04 07:11:53.576400 UTC] Start collecting samples
[2017-11-04 07:11:53.962698 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:53.983641 UTC] Performing policy update
[2017-11-04 07:11:53.984235 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:53.994913 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:54.117039 UTC] Performing line search
[2017-11-04 07:11:54.123932 UTC] Updating baseline
[2017-11-04 07:11:54.284310 UTC] Computing logging information
------------------------------------
| Iteration            | 25        |
| ExpectedImprovement  | 0.012884  |
| ActualImprovement    | 0.0063768 |
| ImprovementRatio     | 0.49494   |
| MeanKL               | 0.0066048 |
| Entropy              | 0.55801   |
| Perplexity           | 1.7472    |
| AveragePolicyProb[0] | 0.49479   |
| AveragePolicyProb[1] | 0.50521   |
| AverageReturn        | 185.03    |
| MinReturn            | 31        |
| MaxReturn            | 200       |
| StdReturn            | 40.624    |
| AverageEpisodeLength | 185.03    |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 40.624    |
| TotalNEpisodes       | 474       |
| TotalNSamples        | 49899     |
| ExplainedVariance    | 0.37006   |
------------------------------------
[2017-11-04 07:11:54.342794 UTC] Saving snapshot
[2017-11-04 07:11:54.350005 UTC] Starting iteration 26
[2017-11-04 07:11:54.350289 UTC] Start collecting samples
[2017-11-04 07:11:54.823543 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:54.840721 UTC] Performing policy update
[2017-11-04 07:11:54.841218 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:54.848704 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:54.939115 UTC] Performing line search
[2017-11-04 07:11:54.947775 UTC] Updating baseline
[2017-11-04 07:11:55.052527 UTC] Computing logging information
------------------------------------
| Iteration            | 26        |
| ExpectedImprovement  | 0.01449   |
| ActualImprovement    | 0.0075307 |
| ImprovementRatio     | 0.51974   |
| MeanKL               | 0.0066631 |
| Entropy              | 0.55278   |
| Perplexity           | 1.7381    |
| AveragePolicyProb[0] | 0.50024   |
| AveragePolicyProb[1] | 0.49976   |
| AverageReturn        | 185.31    |
| MinReturn            | 31        |
| MaxReturn            | 200       |
| StdReturn            | 40.63     |
| AverageEpisodeLength | 185.31    |
| MinEpisodeLength     | 31        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 40.63     |
| TotalNEpisodes       | 487       |
| TotalNSamples        | 52499     |
| ExplainedVariance    | 0.50263   |
------------------------------------
[2017-11-04 07:11:55.098448 UTC] Saving snapshot
[2017-11-04 07:11:55.103434 UTC] Starting iteration 27
[2017-11-04 07:11:55.103688 UTC] Start collecting samples
[2017-11-04 07:11:55.291891 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:55.307091 UTC] Performing policy update
[2017-11-04 07:11:55.307594 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:55.315416 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:55.412340 UTC] Performing line search
[2017-11-04 07:11:55.417574 UTC] Updating baseline
[2017-11-04 07:11:55.520605 UTC] Computing logging information
------------------------------------
| Iteration            | 27        |
| ExpectedImprovement  | 0.019644  |
| ActualImprovement    | 0.015199  |
| ImprovementRatio     | 0.77371   |
| MeanKL               | 0.0079782 |
| Entropy              | 0.55267   |
| Perplexity           | 1.7379    |
| AveragePolicyProb[0] | 0.49994   |
| AveragePolicyProb[1] | 0.50006   |
| AverageReturn        | 189.89    |
| MinReturn            | 42        |
| MaxReturn            | 200       |
| StdReturn            | 33.026    |
| AverageEpisodeLength | 189.89    |
| MinEpisodeLength     | 42        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 33.026    |
| TotalNEpisodes       | 497       |
| TotalNSamples        | 54472     |
| ExplainedVariance    | 0.59356   |
------------------------------------
[2017-11-04 07:11:55.567026 UTC] Saving snapshot
[2017-11-04 07:11:55.572272 UTC] Starting iteration 28
[2017-11-04 07:11:55.572484 UTC] Start collecting samples
[2017-11-04 07:11:55.751198 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:55.767296 UTC] Performing policy update
[2017-11-04 07:11:55.767926 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:55.777828 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:55.870194 UTC] Performing line search
[2017-11-04 07:11:55.876599 UTC] Updating baseline
[2017-11-04 07:11:55.984577 UTC] Computing logging information
------------------------------------
| Iteration            | 28        |
| ExpectedImprovement  | 0.028169  |
| ActualImprovement    | 0.013877  |
| ImprovementRatio     | 0.49265   |
| MeanKL               | 0.0055555 |
| Entropy              | 0.57133   |
| Perplexity           | 1.7706    |
| AveragePolicyProb[0] | 0.50906   |
| AveragePolicyProb[1] | 0.49094   |
| AverageReturn        | 195.52    |
| MinReturn            | 81        |
| MaxReturn            | 200       |
| StdReturn            | 19.331    |
| AverageEpisodeLength | 195.52    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.331    |
| TotalNEpisodes       | 507       |
| TotalNSamples        | 56353     |
| ExplainedVariance    | 0.48635   |
------------------------------------
[2017-11-04 07:11:56.028703 UTC] Saving snapshot
[2017-11-04 07:11:56.033493 UTC] Starting iteration 29
[2017-11-04 07:11:56.033704 UTC] Start collecting samples
[2017-11-04 07:11:56.274401 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:56.289427 UTC] Performing policy update
[2017-11-04 07:11:56.289915 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:56.297457 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:56.388582 UTC] Performing line search
[2017-11-04 07:11:56.394019 UTC] Updating baseline
[2017-11-04 07:11:56.535340 UTC] Computing logging information
------------------------------------
| Iteration            | 29        |
| ExpectedImprovement  | 0.026546  |
| ActualImprovement    | 0.019204  |
| ImprovementRatio     | 0.72343   |
| MeanKL               | 0.0092065 |
| Entropy              | 0.56093   |
| Perplexity           | 1.7523    |
| AveragePolicyProb[0] | 0.51028   |
| AveragePolicyProb[1] | 0.48972   |
| AverageReturn        | 195.66    |
| MinReturn            | 81        |
| MaxReturn            | 200       |
| StdReturn            | 19.923    |
| AverageEpisodeLength | 195.66    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.923    |
| TotalNEpisodes       | 517       |
| TotalNSamples        | 58248     |
| ExplainedVariance    | 0.67187   |
------------------------------------
[2017-11-04 07:11:56.627407 UTC] Saving snapshot
[2017-11-04 07:11:56.635618 UTC] Starting iteration 30
[2017-11-04 07:11:56.635907 UTC] Start collecting samples
[2017-11-04 07:11:57.359232 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:57.391289 UTC] Performing policy update
[2017-11-04 07:11:57.392395 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:57.403780 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:57.558779 UTC] Performing line search
[2017-11-04 07:11:57.567541 UTC] Updating baseline
[2017-11-04 07:11:57.751975 UTC] Computing logging information
------------------------------------
| Iteration            | 30        |
| ExpectedImprovement  | 0.017659  |
| ActualImprovement    | 0.011268  |
| ImprovementRatio     | 0.6381    |
| MeanKL               | 0.0092633 |
| Entropy              | 0.56506   |
| Perplexity           | 1.7595    |
| AveragePolicyProb[0] | 0.48473   |
| AveragePolicyProb[1] | 0.51527   |
| AverageReturn        | 196.17    |
| MinReturn            | 81        |
| MaxReturn            | 200       |
| StdReturn            | 19.502    |
| AverageEpisodeLength | 196.17    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.502    |
| TotalNEpisodes       | 531       |
| TotalNSamples        | 60923     |
| ExplainedVariance    | 0.36825   |
------------------------------------
[2017-11-04 07:11:57.818898 UTC] Saving snapshot
[2017-11-04 07:11:57.827094 UTC] Starting iteration 31
[2017-11-04 07:11:57.827426 UTC] Start collecting samples
[2017-11-04 07:11:58.210475 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:58.225229 UTC] Performing policy update
[2017-11-04 07:11:58.225927 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:58.233609 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:58.321104 UTC] Performing line search
[2017-11-04 07:11:58.326595 UTC] Updating baseline
[2017-11-04 07:11:58.441442 UTC] Computing logging information
------------------------------------
| Iteration            | 31        |
| ExpectedImprovement  | 0.017628  |
| ActualImprovement    | 0.013788  |
| ImprovementRatio     | 0.78218   |
| MeanKL               | 0.0079832 |
| Entropy              | 0.56693   |
| Perplexity           | 1.7628    |
| AveragePolicyProb[0] | 0.50649   |
| AveragePolicyProb[1] | 0.49351   |
| AverageReturn        | 196.17    |
| MinReturn            | 81        |
| MaxReturn            | 200       |
| StdReturn            | 19.502    |
| AverageEpisodeLength | 196.17    |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.502    |
| TotalNEpisodes       | 538       |
| TotalNSamples        | 62323     |
| ExplainedVariance    | 0.64933   |
------------------------------------
[2017-11-04 07:11:58.497119 UTC] Saving snapshot
[2017-11-04 07:11:58.502375 UTC] Starting iteration 32
[2017-11-04 07:11:58.502642 UTC] Start collecting samples
[2017-11-04 07:11:58.834686 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:58.851562 UTC] Performing policy update
[2017-11-04 07:11:58.852172 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:58.860035 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:58.952160 UTC] Performing line search
[2017-11-04 07:11:58.957415 UTC] Updating baseline
[2017-11-04 07:11:59.051188 UTC] Computing logging information
-----------------------------------
| Iteration            | 32       |
| ExpectedImprovement  | 0.023088 |
| ActualImprovement    | 0.01738  |
| ImprovementRatio     | 0.75279  |
| MeanKL               | 0.00899  |
| Entropy              | 0.56388  |
| Perplexity           | 1.7575   |
| AveragePolicyProb[0] | 0.48053  |
| AveragePolicyProb[1] | 0.51947  |
| AverageReturn        | 195.7    |
| MinReturn            | 81       |
| MaxReturn            | 200      |
| StdReturn            | 19.766   |
| AverageEpisodeLength | 195.7    |
| MinEpisodeLength     | 81       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 19.766   |
| TotalNEpisodes       | 549      |
| TotalNSamples        | 64476    |
| ExplainedVariance    | 0.86854  |
-----------------------------------
[2017-11-04 07:11:59.099745 UTC] Saving snapshot
[2017-11-04 07:11:59.104822 UTC] Starting iteration 33
[2017-11-04 07:11:59.105083 UTC] Start collecting samples
[2017-11-04 07:11:59.718159 UTC] Computing input variables for policy optimization
[2017-11-04 07:11:59.747023 UTC] Performing policy update
[2017-11-04 07:11:59.748249 UTC] Computing gradient in Euclidean space
[2017-11-04 07:11:59.762582 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:11:59.896857 UTC] Performing line search
[2017-11-04 07:11:59.908953 UTC] Updating baseline
[2017-11-04 07:12:00.057841 UTC] Computing logging information
------------------------------------
| Iteration            | 33        |
| ExpectedImprovement  | 0.017778  |
| ActualImprovement    | 0.0059843 |
| ImprovementRatio     | 0.33661   |
| MeanKL               | 0.0085321 |
| Entropy              | 0.57482   |
| Perplexity           | 1.7768    |
| AveragePolicyProb[0] | 0.50177   |
| AveragePolicyProb[1] | 0.49823   |
| AverageReturn        | 195.7     |
| MinReturn            | 81        |
| MaxReturn            | 200       |
| StdReturn            | 19.766    |
| AverageEpisodeLength | 195.7     |
| MinEpisodeLength     | 81        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 19.766    |
| TotalNEpisodes       | 558       |
| TotalNSamples        | 66276     |
| ExplainedVariance    | 0.48069   |
------------------------------------
[2017-11-04 07:12:00.156011 UTC] Saving snapshot
[2017-11-04 07:12:00.164792 UTC] Starting iteration 34
[2017-11-04 07:12:00.165322 UTC] Start collecting samples
[2017-11-04 07:12:00.631143 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:00.658753 UTC] Performing policy update
[2017-11-04 07:12:00.659513 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:00.672623 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:00.825002 UTC] Performing line search
[2017-11-04 07:12:00.839485 UTC] Updating baseline
[2017-11-04 07:12:01.007358 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| ExpectedImprovement  | 0.015931  |
| ActualImprovement    | 0.012119  |
| ImprovementRatio     | 0.76071   |
| MeanKL               | 0.0066483 |
| Entropy              | 0.56258   |
| Perplexity           | 1.7552    |
| AveragePolicyProb[0] | 0.50018   |
| AveragePolicyProb[1] | 0.49982   |
| AverageReturn        | 194.07    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 25.292    |
| AverageEpisodeLength | 194.07    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 25.292    |
| TotalNEpisodes       | 569       |
| TotalNSamples        | 68313     |
| ExplainedVariance    | 0.52526   |
------------------------------------
[2017-11-04 07:12:01.075627 UTC] Saving snapshot
[2017-11-04 07:12:01.083147 UTC] Starting iteration 35
[2017-11-04 07:12:01.083427 UTC] Start collecting samples
[2017-11-04 07:12:01.425285 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:01.442049 UTC] Performing policy update
[2017-11-04 07:12:01.442662 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:01.450773 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:01.574052 UTC] Performing line search
[2017-11-04 07:12:01.586152 UTC] Updating baseline
[2017-11-04 07:12:01.725571 UTC] Computing logging information
------------------------------------
| Iteration            | 35        |
| ExpectedImprovement  | 0.01246   |
| ActualImprovement    | 0.0069689 |
| ImprovementRatio     | 0.5593    |
| MeanKL               | 0.0063965 |
| Entropy              | 0.55156   |
| Perplexity           | 1.736     |
| AveragePolicyProb[0] | 0.52513   |
| AveragePolicyProb[1] | 0.47487   |
| AverageReturn        | 194.14    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 25.299    |
| AverageEpisodeLength | 194.14    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 25.299    |
| TotalNEpisodes       | 580       |
| TotalNSamples        | 70513     |
| ExplainedVariance    | 0.69962   |
------------------------------------
[2017-11-04 07:12:01.791814 UTC] Saving snapshot
[2017-11-04 07:12:01.797609 UTC] Starting iteration 36
[2017-11-04 07:12:01.797915 UTC] Start collecting samples
[2017-11-04 07:12:02.080323 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:02.097481 UTC] Performing policy update
[2017-11-04 07:12:02.098008 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:02.105288 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:02.208371 UTC] Performing line search
[2017-11-04 07:12:02.223147 UTC] Updating baseline
[2017-11-04 07:12:02.362138 UTC] Computing logging information
------------------------------------
| Iteration            | 36        |
| ExpectedImprovement  | 0.014329  |
| ActualImprovement    | 0.013467  |
| ImprovementRatio     | 0.93985   |
| MeanKL               | 0.0067534 |
| Entropy              | 0.54776   |
| Perplexity           | 1.7294    |
| AveragePolicyProb[0] | 0.50112   |
| AveragePolicyProb[1] | 0.49888   |
| AverageReturn        | 194.14    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 25.299    |
| AverageEpisodeLength | 194.14    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 25.299    |
| TotalNEpisodes       | 588       |
| TotalNSamples        | 72113     |
| ExplainedVariance    | 0.77381   |
------------------------------------
[2017-11-04 07:12:02.462789 UTC] Saving snapshot
[2017-11-04 07:12:02.473134 UTC] Starting iteration 37
[2017-11-04 07:12:02.473619 UTC] Start collecting samples
[2017-11-04 07:12:03.240894 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:03.273928 UTC] Performing policy update
[2017-11-04 07:12:03.274636 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:03.288187 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:03.455093 UTC] Performing line search
[2017-11-04 07:12:03.464833 UTC] Updating baseline
[2017-11-04 07:12:03.664125 UTC] Computing logging information
------------------------------------
| Iteration            | 37        |
| ExpectedImprovement  | 0.012503  |
| ActualImprovement    | 0.007632  |
| ImprovementRatio     | 0.61043   |
| MeanKL               | 0.0069015 |
| Entropy              | 0.54524   |
| Perplexity           | 1.725     |
| AveragePolicyProb[0] | 0.51442   |
| AveragePolicyProb[1] | 0.48558   |
| AverageReturn        | 194.41    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 25.216    |
| AverageEpisodeLength | 194.41    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 25.216    |
| TotalNEpisodes       | 598       |
| TotalNSamples        | 74113     |
| ExplainedVariance    | 0.71369   |
------------------------------------
[2017-11-04 07:12:03.768716 UTC] Saving snapshot
[2017-11-04 07:12:03.783012 UTC] Starting iteration 38
[2017-11-04 07:12:03.783572 UTC] Start collecting samples
[2017-11-04 07:12:04.372234 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:04.392460 UTC] Performing policy update
[2017-11-04 07:12:04.393154 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:04.402572 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:04.502347 UTC] Performing line search
[2017-11-04 07:12:04.507882 UTC] Updating baseline
[2017-11-04 07:12:04.626974 UTC] Computing logging information
------------------------------------
| Iteration            | 38        |
| ExpectedImprovement  | 0.019495  |
| ActualImprovement    | 0.010156  |
| ImprovementRatio     | 0.52097   |
| MeanKL               | 0.0058065 |
| Entropy              | 0.56024   |
| Perplexity           | 1.7511    |
| AveragePolicyProb[0] | 0.53469   |
| AveragePolicyProb[1] | 0.46531   |
| AverageReturn        | 195.6     |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 22.497    |
| AverageEpisodeLength | 195.6     |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 22.497    |
| TotalNEpisodes       | 612       |
| TotalNSamples        | 76913     |
| ExplainedVariance    | 0.70107   |
------------------------------------
[2017-11-04 07:12:04.687359 UTC] Saving snapshot
[2017-11-04 07:12:04.693144 UTC] Starting iteration 39
[2017-11-04 07:12:04.693410 UTC] Start collecting samples
[2017-11-04 07:12:04.963640 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:04.978708 UTC] Performing policy update
[2017-11-04 07:12:04.979341 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:04.987812 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:05.091490 UTC] Performing line search
[2017-11-04 07:12:05.098764 UTC] Updating baseline
[2017-11-04 07:12:05.246816 UTC] Computing logging information
------------------------------------
| Iteration            | 39        |
| ExpectedImprovement  | 0.022622  |
| ActualImprovement    | 0.017677  |
| ImprovementRatio     | 0.78142   |
| MeanKL               | 0.0096206 |
| Entropy              | 0.55138   |
| Perplexity           | 1.7357    |
| AveragePolicyProb[0] | 0.5015    |
| AveragePolicyProb[1] | 0.4985    |
| AverageReturn        | 196.65    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 20.1      |
| AverageEpisodeLength | 196.65    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 20.1      |
| TotalNEpisodes       | 619       |
| TotalNSamples        | 78313     |
| ExplainedVariance    | 0.64707   |
------------------------------------
[2017-11-04 07:12:05.313677 UTC] Saving snapshot
[2017-11-04 07:12:05.321154 UTC] Starting iteration 40
[2017-11-04 07:12:05.321395 UTC] Start collecting samples
[2017-11-04 07:12:05.571210 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:05.597157 UTC] Performing policy update
[2017-11-04 07:12:05.597848 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:05.609700 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:05.745659 UTC] Performing line search
[2017-11-04 07:12:05.753348 UTC] Updating baseline
[2017-11-04 07:12:05.907329 UTC] Computing logging information
-----------------------------------
| Iteration            | 40       |
| ExpectedImprovement  | 0.017933 |
| ActualImprovement    | 0.014405 |
| ImprovementRatio     | 0.80329  |
| MeanKL               | 0.00876  |
| Entropy              | 0.54603  |
| Perplexity           | 1.7264   |
| AveragePolicyProb[0] | 0.4883   |
| AveragePolicyProb[1] | 0.5117   |
| AverageReturn        | 197.81   |
| MinReturn            | 37       |
| MaxReturn            | 200      |
| StdReturn            | 16.609   |
| AverageEpisodeLength | 197.81   |
| MinEpisodeLength     | 37       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 16.609   |
| TotalNEpisodes       | 629      |
| TotalNSamples        | 80313    |
| ExplainedVariance    | 0.41636  |
-----------------------------------
[2017-11-04 07:12:05.973562 UTC] Saving snapshot
[2017-11-04 07:12:05.981276 UTC] Starting iteration 41
[2017-11-04 07:12:05.981534 UTC] Start collecting samples
[2017-11-04 07:12:06.236197 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:06.251947 UTC] Performing policy update
[2017-11-04 07:12:06.252614 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:06.260098 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:06.352158 UTC] Performing line search
[2017-11-04 07:12:06.357621 UTC] Updating baseline
[2017-11-04 07:12:06.469474 UTC] Computing logging information
------------------------------------
| Iteration            | 41        |
| ExpectedImprovement  | 0.023505  |
| ActualImprovement    | 0.0096614 |
| ImprovementRatio     | 0.41104   |
| MeanKL               | 0.0058535 |
| Entropy              | 0.55323   |
| Perplexity           | 1.7389    |
| AveragePolicyProb[0] | 0.49815   |
| AveragePolicyProb[1] | 0.50185   |
| AverageReturn        | 197.9     |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 16.597    |
| AverageEpisodeLength | 197.9     |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 16.597    |
| TotalNEpisodes       | 639       |
| TotalNSamples        | 82313     |
| ExplainedVariance    | -0.11756  |
------------------------------------
[2017-11-04 07:12:06.528189 UTC] Saving snapshot
[2017-11-04 07:12:06.533585 UTC] Starting iteration 42
[2017-11-04 07:12:06.533921 UTC] Start collecting samples
[2017-11-04 07:12:06.765668 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:06.781824 UTC] Performing policy update
[2017-11-04 07:12:06.782357 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:06.789654 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:06.878766 UTC] Performing line search
[2017-11-04 07:12:06.884233 UTC] Updating baseline
[2017-11-04 07:12:06.979184 UTC] Computing logging information
-------------------------------------
| Iteration            | 42         |
| ExpectedImprovement  | 0.018176   |
| ActualImprovement    | 0.013044   |
| ImprovementRatio     | 0.71766    |
| MeanKL               | 0.0085028  |
| Entropy              | 0.55817    |
| Perplexity           | 1.7475     |
| AveragePolicyProb[0] | 0.51504    |
| AveragePolicyProb[1] | 0.48496    |
| AverageReturn        | 198.37     |
| MinReturn            | 37         |
| MaxReturn            | 200        |
| StdReturn            | 16.218     |
| AverageEpisodeLength | 198.37     |
| MinEpisodeLength     | 37         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 16.218     |
| TotalNEpisodes       | 649        |
| TotalNSamples        | 84313      |
| ExplainedVariance    | 0.00048641 |
-------------------------------------
[2017-11-04 07:12:07.039270 UTC] Saving snapshot
[2017-11-04 07:12:07.044662 UTC] Starting iteration 43
[2017-11-04 07:12:07.044904 UTC] Start collecting samples
[2017-11-04 07:12:07.242891 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:07.270855 UTC] Performing policy update
[2017-11-04 07:12:07.271537 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:07.282267 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:07.408864 UTC] Performing line search
[2017-11-04 07:12:07.416048 UTC] Updating baseline
[2017-11-04 07:12:07.569153 UTC] Computing logging information
------------------------------------
| Iteration            | 43        |
| ExpectedImprovement  | 0.016451  |
| ActualImprovement    | 0.0058358 |
| ImprovementRatio     | 0.35473   |
| MeanKL               | 0.0049204 |
| Entropy              | 0.55533   |
| Perplexity           | 1.7425    |
| AveragePolicyProb[0] | 0.49538   |
| AveragePolicyProb[1] | 0.50462   |
| AverageReturn        | 198.37    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 16.218    |
| AverageEpisodeLength | 198.37    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 16.218    |
| TotalNEpisodes       | 660       |
| TotalNSamples        | 86513     |
| ExplainedVariance    | -0.014644 |
------------------------------------
[2017-11-04 07:12:07.634266 UTC] Saving snapshot
[2017-11-04 07:12:07.642028 UTC] Starting iteration 44
[2017-11-04 07:12:07.642282 UTC] Start collecting samples
[2017-11-04 07:12:07.841425 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:07.857939 UTC] Performing policy update
[2017-11-04 07:12:07.858511 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:07.866126 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:07.960165 UTC] Performing line search
[2017-11-04 07:12:07.965492 UTC] Updating baseline
[2017-11-04 07:12:08.065463 UTC] Computing logging information
------------------------------------
| Iteration            | 44        |
| ExpectedImprovement  | 0.013329  |
| ActualImprovement    | 0.0068977 |
| ImprovementRatio     | 0.5175    |
| MeanKL               | 0.0092422 |
| Entropy              | 0.54612   |
| Perplexity           | 1.7265    |
| AveragePolicyProb[0] | 0.47298   |
| AveragePolicyProb[1] | 0.52702   |
| AverageReturn        | 198.37    |
| MinReturn            | 37        |
| MaxReturn            | 200       |
| StdReturn            | 16.218    |
| AverageEpisodeLength | 198.37    |
| MinEpisodeLength     | 37        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 16.218    |
| TotalNEpisodes       | 668       |
| TotalNSamples        | 88113     |
| ExplainedVariance    | 0.035756  |
------------------------------------
[2017-11-04 07:12:08.127957 UTC] Saving snapshot
[2017-11-04 07:12:08.133460 UTC] Starting iteration 45
[2017-11-04 07:12:08.133719 UTC] Start collecting samples
[2017-11-04 07:12:08.345994 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:08.370140 UTC] Performing policy update
[2017-11-04 07:12:08.370819 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:08.381584 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:08.507768 UTC] Performing line search
[2017-11-04 07:12:08.514974 UTC] Updating baseline
[2017-11-04 07:12:08.663542 UTC] Computing logging information
------------------------------------
| Iteration            | 45        |
| ExpectedImprovement  | 0.017573  |
| ActualImprovement    | 0.0048339 |
| ImprovementRatio     | 0.27507   |
| MeanKL               | 0.0082502 |
| Entropy              | 0.55908   |
| Perplexity           | 1.7491    |
| AveragePolicyProb[0] | 0.49376   |
| AveragePolicyProb[1] | 0.50624   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 678       |
| TotalNSamples        | 90113     |
| ExplainedVariance    | -0.037245 |
------------------------------------
[2017-11-04 07:12:08.728080 UTC] Saving snapshot
[2017-11-04 07:12:08.734480 UTC] Starting iteration 46
[2017-11-04 07:12:08.734713 UTC] Start collecting samples
[2017-11-04 07:12:08.974772 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:09.002473 UTC] Performing policy update
[2017-11-04 07:12:09.003131 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:09.014083 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:09.142302 UTC] Performing line search
[2017-11-04 07:12:09.161122 UTC] Updating baseline
[2017-11-04 07:12:09.310319 UTC] Computing logging information
------------------------------------
| Iteration            | 46        |
| ExpectedImprovement  | 0.010795  |
| ActualImprovement    | 0.0041685 |
| ImprovementRatio     | 0.38615   |
| MeanKL               | 0.0055653 |
| Entropy              | 0.56363   |
| Perplexity           | 1.757     |
| AveragePolicyProb[0] | 0.49894   |
| AveragePolicyProb[1] | 0.50106   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 692       |
| TotalNSamples        | 92913     |
| ExplainedVariance    | 0.27393   |
------------------------------------
[2017-11-04 07:12:09.376935 UTC] Saving snapshot
[2017-11-04 07:12:09.384699 UTC] Starting iteration 47
[2017-11-04 07:12:09.384961 UTC] Start collecting samples
[2017-11-04 07:12:09.562378 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:09.576758 UTC] Performing policy update
[2017-11-04 07:12:09.577307 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:09.585455 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:09.680859 UTC] Performing line search
[2017-11-04 07:12:09.686080 UTC] Updating baseline
[2017-11-04 07:12:09.784879 UTC] Computing logging information
------------------------------------
| Iteration            | 47        |
| ExpectedImprovement  | 0.012924  |
| ActualImprovement    | 0.010985  |
| ImprovementRatio     | 0.84995   |
| MeanKL               | 0.0085971 |
| Entropy              | 0.56298   |
| Perplexity           | 1.7559    |
| AveragePolicyProb[0] | 0.53499   |
| AveragePolicyProb[1] | 0.46501   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 699       |
| TotalNSamples        | 94313     |
| ExplainedVariance    | 0.37988   |
------------------------------------
[2017-11-04 07:12:09.862113 UTC] Saving snapshot
[2017-11-04 07:12:09.869151 UTC] Starting iteration 48
[2017-11-04 07:12:09.869407 UTC] Start collecting samples
[2017-11-04 07:12:10.139253 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:10.156191 UTC] Performing policy update
[2017-11-04 07:12:10.156876 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:10.164869 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:10.257838 UTC] Performing line search
[2017-11-04 07:12:10.266982 UTC] Updating baseline
[2017-11-04 07:12:10.367076 UTC] Computing logging information
------------------------------------
| Iteration            | 48        |
| ExpectedImprovement  | 0.011563  |
| ActualImprovement    | 0.0089459 |
| ImprovementRatio     | 0.77367   |
| MeanKL               | 0.0070724 |
| Entropy              | 0.55085   |
| Perplexity           | 1.7347    |
| AveragePolicyProb[0] | 0.50131   |
| AveragePolicyProb[1] | 0.49869   |
| AverageReturn        | 199.72    |
| MinReturn            | 172       |
| MaxReturn            | 200       |
| StdReturn            | 2.786     |
| AverageEpisodeLength | 199.72    |
| MinEpisodeLength     | 172       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 2.786     |
| TotalNEpisodes       | 709       |
| TotalNSamples        | 96285     |
| ExplainedVariance    | 0.53592   |
------------------------------------
[2017-11-04 07:12:10.423464 UTC] Saving snapshot
[2017-11-04 07:12:10.428676 UTC] Starting iteration 49
[2017-11-04 07:12:10.428892 UTC] Start collecting samples
[2017-11-04 07:12:10.631470 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:10.649271 UTC] Performing policy update
[2017-11-04 07:12:10.649998 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:10.659871 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:10.757123 UTC] Performing line search
[2017-11-04 07:12:10.766182 UTC] Updating baseline
[2017-11-04 07:12:10.864845 UTC] Computing logging information
------------------------------------
| Iteration            | 49        |
| ExpectedImprovement  | 0.010989  |
| ActualImprovement    | 0.0076467 |
| ImprovementRatio     | 0.69586   |
| MeanKL               | 0.0062316 |
| Entropy              | 0.55102   |
| Perplexity           | 1.735     |
| AveragePolicyProb[0] | 0.51913   |
| AveragePolicyProb[1] | 0.48087   |
| AverageReturn        | 199.72    |
| MinReturn            | 172       |
| MaxReturn            | 200       |
| StdReturn            | 2.786     |
| AverageEpisodeLength | 199.72    |
| MinEpisodeLength     | 172       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 2.786     |
| TotalNEpisodes       | 719       |
| TotalNSamples        | 98285     |
| ExplainedVariance    | 0.52698   |
------------------------------------
[2017-11-04 07:12:10.923241 UTC] Saving snapshot
[2017-11-04 07:12:10.928444 UTC] Starting iteration 50
[2017-11-04 07:12:10.928758 UTC] Start collecting samples
[2017-11-04 07:12:11.138919 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:11.154205 UTC] Performing policy update
[2017-11-04 07:12:11.155809 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:11.163850 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:11.254291 UTC] Performing line search
[2017-11-04 07:12:11.263546 UTC] Updating baseline
[2017-11-04 07:12:11.372281 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| ExpectedImprovement  | 0.011246   |
| ActualImprovement    | 0.0095872  |
| ImprovementRatio     | 0.85249    |
| MeanKL               | 0.0094868  |
| Entropy              | 0.53605    |
| Perplexity           | 1.7092     |
| AveragePolicyProb[0] | 0.48141    |
| AveragePolicyProb[1] | 0.51859    |
| AverageReturn        | 199.72     |
| MinReturn            | 172        |
| MaxReturn            | 200        |
| StdReturn            | 2.786      |
| AverageEpisodeLength | 199.72     |
| MinEpisodeLength     | 172        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 2.786      |
| TotalNEpisodes       | 729        |
| TotalNSamples        | 1.0028e+05 |
| ExplainedVariance    | 0.4382     |
-------------------------------------
[2017-11-04 07:12:11.426123 UTC] Saving snapshot
[2017-11-04 07:12:11.431692 UTC] Starting iteration 51
[2017-11-04 07:12:11.431943 UTC] Start collecting samples
[2017-11-04 07:12:11.637247 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:11.654111 UTC] Performing policy update
[2017-11-04 07:12:11.654732 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:11.662576 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:11.751779 UTC] Performing line search
[2017-11-04 07:12:11.757135 UTC] Updating baseline
[2017-11-04 07:12:11.872646 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| ExpectedImprovement  | 0.016629   |
| ActualImprovement    | 0.0095832  |
| ImprovementRatio     | 0.57631    |
| MeanKL               | 0.0083303  |
| Entropy              | 0.53878    |
| Perplexity           | 1.7139     |
| AveragePolicyProb[0] | 0.52239    |
| AveragePolicyProb[1] | 0.47761    |
| AverageReturn        | 199.72     |
| MinReturn            | 172        |
| MaxReturn            | 200        |
| StdReturn            | 2.786      |
| AverageEpisodeLength | 199.72     |
| MinEpisodeLength     | 172        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 2.786      |
| TotalNEpisodes       | 741        |
| TotalNSamples        | 1.0268e+05 |
| ExplainedVariance    | 0.54243    |
-------------------------------------
[2017-11-04 07:12:11.932341 UTC] Saving snapshot
[2017-11-04 07:12:11.937743 UTC] Starting iteration 52
[2017-11-04 07:12:11.937977 UTC] Start collecting samples
[2017-11-04 07:12:12.130017 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:12.144733 UTC] Performing policy update
[2017-11-04 07:12:12.145281 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:12.152797 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:12.245757 UTC] Performing line search
[2017-11-04 07:12:12.252109 UTC] Updating baseline
[2017-11-04 07:12:12.354946 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| ExpectedImprovement  | 0.019256   |
| ActualImprovement    | 0.015594   |
| ImprovementRatio     | 0.80983    |
| MeanKL               | 0.0076905  |
| Entropy              | 0.52088    |
| Perplexity           | 1.6835     |
| AveragePolicyProb[0] | 0.52261    |
| AveragePolicyProb[1] | 0.47739    |
| AverageReturn        | 198.5      |
| MinReturn            | 162        |
| MaxReturn            | 200        |
| StdReturn            | 6.4846     |
| AverageEpisodeLength | 198.5      |
| MinEpisodeLength     | 162        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 6.4846     |
| TotalNEpisodes       | 749        |
| TotalNSamples        | 1.0416e+05 |
| ExplainedVariance    | 0.77098    |
-------------------------------------
[2017-11-04 07:12:12.416785 UTC] Saving snapshot
[2017-11-04 07:12:12.422105 UTC] Starting iteration 53
[2017-11-04 07:12:12.422362 UTC] Start collecting samples
[2017-11-04 07:12:12.635288 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:12.653165 UTC] Performing policy update
[2017-11-04 07:12:12.653790 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:12.661686 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:12.752013 UTC] Performing line search
[2017-11-04 07:12:12.757696 UTC] Updating baseline
[2017-11-04 07:12:12.862015 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| ExpectedImprovement  | 0.012251   |
| ActualImprovement    | 0.007814   |
| ImprovementRatio     | 0.63783    |
| MeanKL               | 0.0086607  |
| Entropy              | 0.52798    |
| Perplexity           | 1.6955     |
| AveragePolicyProb[0] | 0.51513    |
| AveragePolicyProb[1] | 0.48487    |
| AverageReturn        | 198.23     |
| MinReturn            | 162        |
| MaxReturn            | 200        |
| StdReturn            | 6.5603     |
| AverageEpisodeLength | 198.23     |
| MinEpisodeLength     | 162        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 6.5603     |
| TotalNEpisodes       | 761        |
| TotalNSamples        | 1.0654e+05 |
| ExplainedVariance    | 0.67165    |
-------------------------------------
[2017-11-04 07:12:12.916988 UTC] Saving snapshot
[2017-11-04 07:12:12.922133 UTC] Starting iteration 54
[2017-11-04 07:12:12.922371 UTC] Start collecting samples
[2017-11-04 07:12:13.121582 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:13.140493 UTC] Performing policy update
[2017-11-04 07:12:13.141180 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:13.149132 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:13.241872 UTC] Performing line search
[2017-11-04 07:12:13.247579 UTC] Updating baseline
[2017-11-04 07:12:13.362617 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| ExpectedImprovement  | 0.016904   |
| ActualImprovement    | 0.011804   |
| ImprovementRatio     | 0.69827    |
| MeanKL               | 0.0087167  |
| Entropy              | 0.5209     |
| Perplexity           | 1.6835     |
| AveragePolicyProb[0] | 0.51612    |
| AveragePolicyProb[1] | 0.48388    |
| AverageReturn        | 197.47     |
| MinReturn            | 161        |
| MaxReturn            | 200        |
| StdReturn            | 8.2867     |
| AverageEpisodeLength | 197.47     |
| MinEpisodeLength     | 161        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 8.2867     |
| TotalNEpisodes       | 773        |
| TotalNSamples        | 1.0886e+05 |
| ExplainedVariance    | 0.23051    |
-------------------------------------
[2017-11-04 07:12:13.424872 UTC] Saving snapshot
[2017-11-04 07:12:13.430466 UTC] Starting iteration 55
[2017-11-04 07:12:13.430745 UTC] Start collecting samples
[2017-11-04 07:12:13.633482 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:13.648930 UTC] Performing policy update
[2017-11-04 07:12:13.649632 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:13.657609 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:13.751411 UTC] Performing line search
[2017-11-04 07:12:13.761000 UTC] Updating baseline
[2017-11-04 07:12:13.861875 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| ExpectedImprovement  | 0.014655   |
| ActualImprovement    | 0.0067365  |
| ImprovementRatio     | 0.45969    |
| MeanKL               | 0.0069697  |
| Entropy              | 0.53651    |
| Perplexity           | 1.71       |
| AveragePolicyProb[0] | 0.48978    |
| AveragePolicyProb[1] | 0.51022    |
| AverageReturn        | 197.12     |
| MinReturn            | 161        |
| MaxReturn            | 200        |
| StdReturn            | 8.8151     |
| AverageEpisodeLength | 197.12     |
| MinEpisodeLength     | 161        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 8.8151     |
| TotalNEpisodes       | 781        |
| TotalNSamples        | 1.1042e+05 |
| ExplainedVariance    | 0.5362     |
-------------------------------------
[2017-11-04 07:12:13.923501 UTC] Saving snapshot
[2017-11-04 07:12:13.929409 UTC] Starting iteration 56
[2017-11-04 07:12:13.929707 UTC] Start collecting samples
[2017-11-04 07:12:14.131421 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:14.148563 UTC] Performing policy update
[2017-11-04 07:12:14.149208 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:14.157183 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:14.256858 UTC] Performing line search
[2017-11-04 07:12:14.262493 UTC] Updating baseline
[2017-11-04 07:12:14.384972 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| ExpectedImprovement  | 0.015775   |
| ActualImprovement    | 0.004641   |
| ImprovementRatio     | 0.29419    |
| MeanKL               | 0.0069385  |
| Entropy              | 0.5395     |
| Perplexity           | 1.7151     |
| AveragePolicyProb[0] | 0.50909    |
| AveragePolicyProb[1] | 0.49091    |
| AverageReturn        | 195.8      |
| MinReturn            | 130        |
| MaxReturn            | 200        |
| StdReturn            | 11.577     |
| AverageEpisodeLength | 195.8      |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 11.577     |
| TotalNEpisodes       | 792        |
| TotalNSamples        | 1.1249e+05 |
| ExplainedVariance    | 0.58262    |
-------------------------------------
[2017-11-04 07:12:14.446130 UTC] Saving snapshot
[2017-11-04 07:12:14.451333 UTC] Starting iteration 57
[2017-11-04 07:12:14.451717 UTC] Start collecting samples
[2017-11-04 07:12:14.673039 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:14.689760 UTC] Performing policy update
[2017-11-04 07:12:14.690424 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:14.698462 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:14.792622 UTC] Performing line search
[2017-11-04 07:12:14.798248 UTC] Updating baseline
[2017-11-04 07:12:14.901776 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| ExpectedImprovement  | 0.017491   |
| ActualImprovement    | 0.010513   |
| ImprovementRatio     | 0.60107    |
| MeanKL               | 0.0059624  |
| Entropy              | 0.54231    |
| Perplexity           | 1.72       |
| AveragePolicyProb[0] | 0.51119    |
| AveragePolicyProb[1] | 0.48881    |
| AverageReturn        | 195.18     |
| MinReturn            | 130        |
| MaxReturn            | 200        |
| StdReturn            | 12.172     |
| AverageEpisodeLength | 195.18     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 12.172     |
| TotalNEpisodes       | 803        |
| TotalNSamples        | 1.1463e+05 |
| ExplainedVariance    | 0.66622    |
-------------------------------------
[2017-11-04 07:12:14.961792 UTC] Saving snapshot
[2017-11-04 07:12:14.967353 UTC] Starting iteration 58
[2017-11-04 07:12:14.967639 UTC] Start collecting samples
[2017-11-04 07:12:15.180268 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:15.204754 UTC] Performing policy update
[2017-11-04 07:12:15.205468 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:15.216537 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:15.349011 UTC] Performing line search
[2017-11-04 07:12:15.362636 UTC] Updating baseline
[2017-11-04 07:12:15.516692 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| ExpectedImprovement  | 0.015604   |
| ActualImprovement    | 0.013122   |
| ImprovementRatio     | 0.84095    |
| MeanKL               | 0.0067671  |
| Entropy              | 0.54842    |
| Perplexity           | 1.7305     |
| AveragePolicyProb[0] | 0.51451    |
| AveragePolicyProb[1] | 0.48549    |
| AverageReturn        | 193.92     |
| MinReturn            | 130        |
| MaxReturn            | 200        |
| StdReturn            | 14.557     |
| AverageEpisodeLength | 193.92     |
| MinEpisodeLength     | 130        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 14.557     |
| TotalNEpisodes       | 813        |
| TotalNSamples        | 1.1648e+05 |
| ExplainedVariance    | 0.73504    |
-------------------------------------
[2017-11-04 07:12:15.587070 UTC] Saving snapshot
[2017-11-04 07:12:15.594816 UTC] Starting iteration 59
[2017-11-04 07:12:15.595089 UTC] Start collecting samples
[2017-11-04 07:12:15.798651 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:15.814544 UTC] Performing policy update
[2017-11-04 07:12:15.815146 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:15.822910 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:15.913565 UTC] Performing line search
[2017-11-04 07:12:15.922360 UTC] Updating baseline
[2017-11-04 07:12:16.019448 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| ExpectedImprovement  | 0.015657   |
| ActualImprovement    | 0.010535   |
| ImprovementRatio     | 0.67286    |
| MeanKL               | 0.0074789  |
| Entropy              | 0.54148    |
| Perplexity           | 1.7186     |
| AveragePolicyProb[0] | 0.51423    |
| AveragePolicyProb[1] | 0.48577    |
| AverageReturn        | 191.5      |
| MinReturn            | 125        |
| MaxReturn            | 200        |
| StdReturn            | 17.527     |
| AverageEpisodeLength | 191.5      |
| MinEpisodeLength     | 125        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 17.527     |
| TotalNEpisodes       | 823        |
| TotalNSamples        | 1.1824e+05 |
| ExplainedVariance    | 0.85372    |
-------------------------------------
[2017-11-04 07:12:16.072941 UTC] Saving snapshot
[2017-11-04 07:12:16.078102 UTC] Starting iteration 60
[2017-11-04 07:12:16.078310 UTC] Start collecting samples
[2017-11-04 07:12:16.273731 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:16.291136 UTC] Performing policy update
[2017-11-04 07:12:16.291720 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:16.299554 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:16.391289 UTC] Performing line search
[2017-11-04 07:12:16.396916 UTC] Updating baseline
[2017-11-04 07:12:16.506698 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| ExpectedImprovement  | 0.018064   |
| ActualImprovement    | 0.011312   |
| ImprovementRatio     | 0.62622    |
| MeanKL               | 0.0076683  |
| Entropy              | 0.5263     |
| Perplexity           | 1.6927     |
| AveragePolicyProb[0] | 0.53036    |
| AveragePolicyProb[1] | 0.46964    |
| AverageReturn        | 190.06     |
| MinReturn            | 125        |
| MaxReturn            | 200        |
| StdReturn            | 18.97      |
| AverageEpisodeLength | 190.06     |
| MinEpisodeLength     | 125        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 18.97      |
| TotalNEpisodes       | 835        |
| TotalNSamples        | 1.2049e+05 |
| ExplainedVariance    | 0.47806    |
-------------------------------------
[2017-11-04 07:12:16.563809 UTC] Saving snapshot
[2017-11-04 07:12:16.570026 UTC] Starting iteration 61
[2017-11-04 07:12:16.570325 UTC] Start collecting samples
[2017-11-04 07:12:16.771278 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:16.790441 UTC] Performing policy update
[2017-11-04 07:12:16.791058 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:16.798732 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:16.888216 UTC] Performing line search
[2017-11-04 07:12:16.897226 UTC] Updating baseline
[2017-11-04 07:12:16.996806 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| ExpectedImprovement  | 0.016987   |
| ActualImprovement    | 0.015543   |
| ImprovementRatio     | 0.91498    |
| MeanKL               | 0.006573   |
| Entropy              | 0.52002    |
| Perplexity           | 1.6821     |
| AveragePolicyProb[0] | 0.51411    |
| AveragePolicyProb[1] | 0.48589    |
| AverageReturn        | 188.19     |
| MinReturn            | 125        |
| MaxReturn            | 200        |
| StdReturn            | 21.274     |
| AverageEpisodeLength | 188.19     |
| MinEpisodeLength     | 125        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 21.274     |
| TotalNEpisodes       | 847        |
| TotalNSamples        | 1.2262e+05 |
| ExplainedVariance    | 0.84772    |
-------------------------------------
[2017-11-04 07:12:17.051594 UTC] Saving snapshot
[2017-11-04 07:12:17.057042 UTC] Starting iteration 62
[2017-11-04 07:12:17.057267 UTC] Start collecting samples
[2017-11-04 07:12:17.248214 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:17.264707 UTC] Performing policy update
[2017-11-04 07:12:17.265360 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:17.273171 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:17.367143 UTC] Performing line search
[2017-11-04 07:12:17.372334 UTC] Updating baseline
[2017-11-04 07:12:17.474613 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| ExpectedImprovement  | 0.014677   |
| ActualImprovement    | 0.0049993  |
| ImprovementRatio     | 0.34063    |
| MeanKL               | 0.0052981  |
| Entropy              | 0.51901    |
| Perplexity           | 1.6804     |
| AveragePolicyProb[0] | 0.51688    |
| AveragePolicyProb[1] | 0.48312    |
| AverageReturn        | 187.19     |
| MinReturn            | 122        |
| MaxReturn            | 200        |
| StdReturn            | 22.722     |
| AverageEpisodeLength | 187.19     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 22.722     |
| TotalNEpisodes       | 857        |
| TotalNSamples        | 1.2447e+05 |
| ExplainedVariance    | 0.58532    |
-------------------------------------
[2017-11-04 07:12:17.526173 UTC] Saving snapshot
[2017-11-04 07:12:17.531254 UTC] Starting iteration 63
[2017-11-04 07:12:17.531484 UTC] Start collecting samples
[2017-11-04 07:12:17.717561 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:17.734313 UTC] Performing policy update
[2017-11-04 07:12:17.734997 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:17.743129 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:17.837185 UTC] Performing line search
[2017-11-04 07:12:17.842623 UTC] Updating baseline
[2017-11-04 07:12:17.944897 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| ExpectedImprovement  | 0.019167   |
| ActualImprovement    | 0.015687   |
| ImprovementRatio     | 0.81844    |
| MeanKL               | 0.0085624  |
| Entropy              | 0.52005    |
| Perplexity           | 1.6821     |
| AveragePolicyProb[0] | 0.50731    |
| AveragePolicyProb[1] | 0.49269    |
| AverageReturn        | 186.43     |
| MinReturn            | 122        |
| MaxReturn            | 200        |
| StdReturn            | 23.533     |
| AverageEpisodeLength | 186.43     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.533     |
| TotalNEpisodes       | 865        |
| TotalNSamples        | 1.2594e+05 |
| ExplainedVariance    | 0.63061    |
-------------------------------------
[2017-11-04 07:12:17.996281 UTC] Saving snapshot
[2017-11-04 07:12:18.001344 UTC] Starting iteration 64
[2017-11-04 07:12:18.001559 UTC] Start collecting samples
[2017-11-04 07:12:18.195911 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:18.214141 UTC] Performing policy update
[2017-11-04 07:12:18.214750 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:18.222126 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:18.298258 UTC] Performing line search
[2017-11-04 07:12:18.302966 UTC] Updating baseline
[2017-11-04 07:12:18.397858 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| ExpectedImprovement  | 0.017646   |
| ActualImprovement    | 0.0046205  |
| ImprovementRatio     | 0.26184    |
| MeanKL               | 0.0055032  |
| Entropy              | 0.52854    |
| Perplexity           | 1.6964     |
| AveragePolicyProb[0] | 0.53166    |
| AveragePolicyProb[1] | 0.46834    |
| AverageReturn        | 186.14     |
| MinReturn            | 122        |
| MaxReturn            | 200        |
| StdReturn            | 23.688     |
| AverageEpisodeLength | 186.14     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.688     |
| TotalNEpisodes       | 881        |
| TotalNSamples        | 1.2904e+05 |
| ExplainedVariance    | 0.5951     |
-------------------------------------
[2017-11-04 07:12:18.458223 UTC] Saving snapshot
[2017-11-04 07:12:18.463430 UTC] Starting iteration 65
[2017-11-04 07:12:18.463662 UTC] Start collecting samples
[2017-11-04 07:12:18.707684 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:18.731121 UTC] Performing policy update
[2017-11-04 07:12:18.732596 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:18.742980 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:18.863988 UTC] Performing line search
[2017-11-04 07:12:18.871017 UTC] Updating baseline
[2017-11-04 07:12:19.024220 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| ExpectedImprovement  | 0.014856   |
| ActualImprovement    | 0.0079764  |
| ImprovementRatio     | 0.53692    |
| MeanKL               | 0.0089926  |
| Entropy              | 0.52658    |
| Perplexity           | 1.6931     |
| AveragePolicyProb[0] | 0.51365    |
| AveragePolicyProb[1] | 0.48635    |
| AverageReturn        | 185.32     |
| MinReturn            | 122        |
| MaxReturn            | 200        |
| StdReturn            | 24.379     |
| AverageEpisodeLength | 185.32     |
| MinEpisodeLength     | 122        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 24.379     |
| TotalNEpisodes       | 889        |
| TotalNSamples        | 1.3053e+05 |
| ExplainedVariance    | 0.45918    |
-------------------------------------
[2017-11-04 07:12:19.092364 UTC] Saving snapshot
[2017-11-04 07:12:19.099792 UTC] Starting iteration 66
[2017-11-04 07:12:19.100052 UTC] Start collecting samples
[2017-11-04 07:12:19.316369 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:19.332513 UTC] Performing policy update
[2017-11-04 07:12:19.333049 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:19.341512 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:19.432066 UTC] Performing line search
[2017-11-04 07:12:19.437171 UTC] Updating baseline
[2017-11-04 07:12:19.537494 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| ExpectedImprovement  | 0.021387   |
| ActualImprovement    | 0.014213   |
| ImprovementRatio     | 0.66457    |
| MeanKL               | 0.0084726  |
| Entropy              | 0.52036    |
| Perplexity           | 1.6826     |
| AveragePolicyProb[0] | 0.51188    |
| AveragePolicyProb[1] | 0.48812    |
| AverageReturn        | 185.35     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 26.712     |
| AverageEpisodeLength | 185.35     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 26.712     |
| TotalNEpisodes       | 898        |
| TotalNSamples        | 1.3219e+05 |
| ExplainedVariance    | 0.31613    |
-------------------------------------
[2017-11-04 07:12:19.594708 UTC] Saving snapshot
[2017-11-04 07:12:19.599938 UTC] Starting iteration 67
[2017-11-04 07:12:19.600157 UTC] Start collecting samples
[2017-11-04 07:12:19.790615 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:19.808933 UTC] Performing policy update
[2017-11-04 07:12:19.809423 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:19.816893 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:19.905884 UTC] Performing line search
[2017-11-04 07:12:19.911163 UTC] Updating baseline
[2017-11-04 07:12:20.007785 UTC] Computing logging information
-------------------------------------
| Iteration            | 67         |
| ExpectedImprovement  | 0.020098   |
| ActualImprovement    | 0.015925   |
| ImprovementRatio     | 0.79238    |
| MeanKL               | 0.0087544  |
| Entropy              | 0.52688    |
| Perplexity           | 1.6936     |
| AveragePolicyProb[0] | 0.48727    |
| AveragePolicyProb[1] | 0.51273    |
| AverageReturn        | 185.77     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 26.425     |
| AverageEpisodeLength | 185.77     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 26.425     |
| TotalNEpisodes       | 909        |
| TotalNSamples        | 1.3432e+05 |
| ExplainedVariance    | 0.25707    |
-------------------------------------
[2017-11-04 07:12:20.070099 UTC] Saving snapshot
[2017-11-04 07:12:20.075109 UTC] Starting iteration 68
[2017-11-04 07:12:20.075332 UTC] Start collecting samples
[2017-11-04 07:12:20.331644 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:20.357288 UTC] Performing policy update
[2017-11-04 07:12:20.358003 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:20.369247 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:20.500583 UTC] Performing line search
[2017-11-04 07:12:20.507838 UTC] Updating baseline
[2017-11-04 07:12:20.667541 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| ExpectedImprovement  | 0.018206   |
| ActualImprovement    | 0.012716   |
| ImprovementRatio     | 0.69846    |
| MeanKL               | 0.0092145  |
| Entropy              | 0.51951    |
| Perplexity           | 1.6812     |
| AveragePolicyProb[0] | 0.50753    |
| AveragePolicyProb[1] | 0.49247    |
| AverageReturn        | 188.09     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 25.345     |
| AverageEpisodeLength | 188.09     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 25.345     |
| TotalNEpisodes       | 920        |
| TotalNSamples        | 1.3652e+05 |
| ExplainedVariance    | 0.35225    |
-------------------------------------
[2017-11-04 07:12:20.748491 UTC] Saving snapshot
[2017-11-04 07:12:20.756644 UTC] Starting iteration 69
[2017-11-04 07:12:20.756965 UTC] Start collecting samples
[2017-11-04 07:12:21.035885 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:21.052701 UTC] Performing policy update
[2017-11-04 07:12:21.053363 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:21.062067 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:21.158711 UTC] Performing line search
[2017-11-04 07:12:21.172733 UTC] Updating baseline
[2017-11-04 07:12:21.286074 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| ExpectedImprovement  | 0.0098564  |
| ActualImprovement    | 0.0061749  |
| ImprovementRatio     | 0.62649    |
| MeanKL               | 0.0060271  |
| Entropy              | 0.51633    |
| Perplexity           | 1.6759     |
| AveragePolicyProb[0] | 0.51032    |
| AveragePolicyProb[1] | 0.48968    |
| AverageReturn        | 190.04     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 23.565     |
| AverageEpisodeLength | 190.04     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.565     |
| TotalNEpisodes       | 929        |
| TotalNSamples        | 1.3832e+05 |
| ExplainedVariance    | 0.10489    |
-------------------------------------
[2017-11-04 07:12:21.342359 UTC] Saving snapshot
[2017-11-04 07:12:21.348712 UTC] Starting iteration 70
[2017-11-04 07:12:21.348977 UTC] Start collecting samples
[2017-11-04 07:12:21.816790 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:21.834669 UTC] Performing policy update
[2017-11-04 07:12:21.835216 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:21.842992 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:21.940495 UTC] Performing line search
[2017-11-04 07:12:21.945856 UTC] Updating baseline
[2017-11-04 07:12:22.060854 UTC] Computing logging information
-------------------------------------
| Iteration            | 70         |
| ExpectedImprovement  | 0.015426   |
| ActualImprovement    | 0.0081656  |
| ImprovementRatio     | 0.52935    |
| MeanKL               | 0.0063408  |
| Entropy              | 0.50655    |
| Perplexity           | 1.6596     |
| AveragePolicyProb[0] | 0.50989    |
| AveragePolicyProb[1] | 0.49011    |
| AverageReturn        | 190.86     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 23.056     |
| AverageEpisodeLength | 190.86     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.056     |
| TotalNEpisodes       | 940        |
| TotalNSamples        | 1.4052e+05 |
| ExplainedVariance    | 0.050896   |
-------------------------------------
[2017-11-04 07:12:22.120980 UTC] Saving snapshot
[2017-11-04 07:12:22.126261 UTC] Starting iteration 71
[2017-11-04 07:12:22.126528 UTC] Start collecting samples
[2017-11-04 07:12:22.314704 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:22.332118 UTC] Performing policy update
[2017-11-04 07:12:22.332761 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:22.340578 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:22.430798 UTC] Performing line search
[2017-11-04 07:12:22.436273 UTC] Updating baseline
[2017-11-04 07:12:22.533426 UTC] Computing logging information
------------------------------------
| Iteration            | 71        |
| ExpectedImprovement  | 0.01846   |
| ActualImprovement    | 0.0080566 |
| ImprovementRatio     | 0.43643   |
| MeanKL               | 0.0060718 |
| Entropy              | 0.50999   |
| Perplexity           | 1.6653    |
| AveragePolicyProb[0] | 0.51073   |
| AveragePolicyProb[1] | 0.48927   |
| AverageReturn        | 192.75    |
| MinReturn            | 61        |
| MaxReturn            | 200       |
| StdReturn            | 21.093    |
| AverageEpisodeLength | 192.75    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 21.093    |
| TotalNEpisodes       | 948       |
| TotalNSamples        | 1.421e+05 |
| ExplainedVariance    | 0.43766   |
------------------------------------
[2017-11-04 07:12:22.589387 UTC] Saving snapshot
[2017-11-04 07:12:22.594728 UTC] Starting iteration 72
[2017-11-04 07:12:22.594939 UTC] Start collecting samples
[2017-11-04 07:12:22.793644 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:22.811650 UTC] Performing policy update
[2017-11-04 07:12:22.812193 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:22.819572 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:22.908738 UTC] Performing line search
[2017-11-04 07:12:22.913659 UTC] Updating baseline
[2017-11-04 07:12:23.008685 UTC] Computing logging information
------------------------------------
| Iteration            | 72        |
| ExpectedImprovement  | 0.011584  |
| ActualImprovement    | 0.0071908 |
| ImprovementRatio     | 0.62077   |
| MeanKL               | 0.0081398 |
| Entropy              | 0.52761   |
| Perplexity           | 1.6949    |
| AveragePolicyProb[0] | 0.51518   |
| AveragePolicyProb[1] | 0.48482   |
| AverageReturn        | 194.85    |
| MinReturn            | 61        |
| MaxReturn            | 200       |
| StdReturn            | 18.407    |
| AverageEpisodeLength | 194.85    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 18.407    |
| TotalNEpisodes       | 961       |
| TotalNSamples        | 1.447e+05 |
| ExplainedVariance    | 0.13754   |
------------------------------------
[2017-11-04 07:12:23.070720 UTC] Saving snapshot
[2017-11-04 07:12:23.076763 UTC] Starting iteration 73
[2017-11-04 07:12:23.076990 UTC] Start collecting samples
[2017-11-04 07:12:23.270747 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:23.285906 UTC] Performing policy update
[2017-11-04 07:12:23.286489 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:23.293887 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:23.383979 UTC] Performing line search
[2017-11-04 07:12:23.389166 UTC] Updating baseline
[2017-11-04 07:12:23.493594 UTC] Computing logging information
------------------------------------
| Iteration            | 73        |
| ExpectedImprovement  | 0.01395   |
| ActualImprovement    | 0.010135  |
| ImprovementRatio     | 0.72651   |
| MeanKL               | 0.0085744 |
| Entropy              | 0.50437   |
| Perplexity           | 1.6559    |
| AveragePolicyProb[0] | 0.49456   |
| AveragePolicyProb[1] | 0.50544   |
| AverageReturn        | 195.75    |
| MinReturn            | 61        |
| MaxReturn            | 200       |
| StdReturn            | 17.293    |
| AverageEpisodeLength | 195.75    |
| MinEpisodeLength     | 61        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 17.293    |
| TotalNEpisodes       | 970       |
| TotalNSamples        | 1.465e+05 |
| ExplainedVariance    | 0.15996   |
------------------------------------
[2017-11-04 07:12:23.556233 UTC] Saving snapshot
[2017-11-04 07:12:23.561510 UTC] Starting iteration 74
[2017-11-04 07:12:23.561736 UTC] Start collecting samples
[2017-11-04 07:12:23.756381 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:23.772251 UTC] Performing policy update
[2017-11-04 07:12:23.772800 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:23.780642 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:23.871293 UTC] Performing line search
[2017-11-04 07:12:23.880284 UTC] Updating baseline
[2017-11-04 07:12:23.977801 UTC] Computing logging information
-------------------------------------
| Iteration            | 74         |
| ExpectedImprovement  | 0.013614   |
| ActualImprovement    | 0.011043   |
| ImprovementRatio     | 0.81115    |
| MeanKL               | 0.0066355  |
| Entropy              | 0.52979    |
| Perplexity           | 1.6986     |
| AveragePolicyProb[0] | 0.48987    |
| AveragePolicyProb[1] | 0.51013    |
| AverageReturn        | 196.35     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 16.481     |
| AverageEpisodeLength | 196.35     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 16.481     |
| TotalNEpisodes       | 979        |
| TotalNSamples        | 1.4827e+05 |
| ExplainedVariance    | 0.36115    |
-------------------------------------
[2017-11-04 07:12:24.037475 UTC] Saving snapshot
[2017-11-04 07:12:24.043019 UTC] Starting iteration 75
[2017-11-04 07:12:24.043231 UTC] Start collecting samples
[2017-11-04 07:12:24.243679 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:24.261025 UTC] Performing policy update
[2017-11-04 07:12:24.261531 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:24.268748 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:24.363343 UTC] Performing line search
[2017-11-04 07:12:24.368832 UTC] Updating baseline
[2017-11-04 07:12:24.486172 UTC] Computing logging information
-------------------------------------
| Iteration            | 75         |
| ExpectedImprovement  | 0.014007   |
| ActualImprovement    | 0.0075685  |
| ImprovementRatio     | 0.54033    |
| MeanKL               | 0.0076573  |
| Entropy              | 0.51897    |
| Perplexity           | 1.6803     |
| AveragePolicyProb[0] | 0.50144    |
| AveragePolicyProb[1] | 0.49856    |
| AverageReturn        | 197.43     |
| MinReturn            | 61         |
| MaxReturn            | 200        |
| StdReturn            | 14.766     |
| AverageEpisodeLength | 197.43     |
| MinEpisodeLength     | 61         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 14.766     |
| TotalNEpisodes       | 989        |
| TotalNSamples        | 1.5027e+05 |
| ExplainedVariance    | -0.034565  |
-------------------------------------
[2017-11-04 07:12:24.546246 UTC] Saving snapshot
[2017-11-04 07:12:24.551562 UTC] Starting iteration 76
[2017-11-04 07:12:24.551767 UTC] Start collecting samples
[2017-11-04 07:12:24.776499 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:24.793521 UTC] Performing policy update
[2017-11-04 07:12:24.794203 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:24.803190 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:24.895489 UTC] Performing line search
[2017-11-04 07:12:24.902603 UTC] Updating baseline
[2017-11-04 07:12:25.024712 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| ExpectedImprovement  | 0.014941   |
| ActualImprovement    | 0.0091977  |
| ImprovementRatio     | 0.61558    |
| MeanKL               | 0.0094728  |
| Entropy              | 0.5222     |
| Perplexity           | 1.6857     |
| AveragePolicyProb[0] | 0.5098     |
| AveragePolicyProb[1] | 0.4902     |
| AverageReturn        | 199.13     |
| MinReturn            | 158        |
| MaxReturn            | 200        |
| StdReturn            | 5.0807     |
| AverageEpisodeLength | 199.13     |
| MinEpisodeLength     | 158        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 5.0807     |
| TotalNEpisodes       | 1000       |
| TotalNSamples        | 1.5247e+05 |
| ExplainedVariance    | 0.12059    |
-------------------------------------
[2017-11-04 07:12:25.083155 UTC] Saving snapshot
[2017-11-04 07:12:25.088491 UTC] Starting iteration 77
[2017-11-04 07:12:25.088713 UTC] Start collecting samples
[2017-11-04 07:12:25.304752 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:25.321618 UTC] Performing policy update
[2017-11-04 07:12:25.322247 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:25.330260 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:25.412118 UTC] Performing line search
[2017-11-04 07:12:25.417250 UTC] Updating baseline
[2017-11-04 07:12:25.526442 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| ExpectedImprovement  | 0.011817   |
| ActualImprovement    | 0.0082357  |
| ImprovementRatio     | 0.69696    |
| MeanKL               | 0.0095173  |
| Entropy              | 0.51076    |
| Perplexity           | 1.6666     |
| AveragePolicyProb[0] | 0.50676    |
| AveragePolicyProb[1] | 0.49324    |
| AverageReturn        | 199.55     |
| MinReturn            | 178        |
| MaxReturn            | 200        |
| StdReturn            | 2.9542     |
| AverageEpisodeLength | 199.55     |
| MinEpisodeLength     | 178        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 2.9542     |
| TotalNEpisodes       | 1010       |
| TotalNSamples        | 1.5447e+05 |
| ExplainedVariance    | 0.17903    |
-------------------------------------
[2017-11-04 07:12:25.580140 UTC] Saving snapshot
[2017-11-04 07:12:25.585230 UTC] Starting iteration 78
[2017-11-04 07:12:25.585431 UTC] Start collecting samples
[2017-11-04 07:12:25.778911 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:25.795033 UTC] Performing policy update
[2017-11-04 07:12:25.795603 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:25.803186 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:25.896138 UTC] Performing line search
[2017-11-04 07:12:25.901543 UTC] Updating baseline
[2017-11-04 07:12:26.044167 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| ExpectedImprovement  | 0.01495    |
| ActualImprovement    | 0.010056   |
| ImprovementRatio     | 0.67265    |
| MeanKL               | 0.0084945  |
| Entropy              | 0.50648    |
| Perplexity           | 1.6594     |
| AveragePolicyProb[0] | 0.51095    |
| AveragePolicyProb[1] | 0.48905    |
| AverageReturn        | 199.55     |
| MinReturn            | 178        |
| MaxReturn            | 200        |
| StdReturn            | 2.9542     |
| AverageEpisodeLength | 199.55     |
| MinEpisodeLength     | 178        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 2.9542     |
| TotalNEpisodes       | 1020       |
| TotalNSamples        | 1.5647e+05 |
| ExplainedVariance    | 0.27387    |
-------------------------------------
[2017-11-04 07:12:26.107897 UTC] Saving snapshot
[2017-11-04 07:12:26.114013 UTC] Starting iteration 79
[2017-11-04 07:12:26.114413 UTC] Start collecting samples
[2017-11-04 07:12:26.940259 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:26.966708 UTC] Performing policy update
[2017-11-04 07:12:26.967276 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:26.978649 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:27.108978 UTC] Performing line search
[2017-11-04 07:12:27.116303 UTC] Updating baseline
[2017-11-04 07:12:27.262577 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| ExpectedImprovement  | 0.018229   |
| ActualImprovement    | 0.013714   |
| ImprovementRatio     | 0.75236    |
| MeanKL               | 0.0092453  |
| Entropy              | 0.51132    |
| Perplexity           | 1.6675     |
| AveragePolicyProb[0] | 0.51938    |
| AveragePolicyProb[1] | 0.48062    |
| AverageReturn        | 198.71     |
| MinReturn            | 168        |
| MaxReturn            | 200        |
| StdReturn            | 5.5701     |
| AverageEpisodeLength | 198.71     |
| MinEpisodeLength     | 168        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 5.5701     |
| TotalNEpisodes       | 1030       |
| TotalNSamples        | 1.5839e+05 |
| ExplainedVariance    | 0.57926    |
-------------------------------------
[2017-11-04 07:12:27.327477 UTC] Saving snapshot
[2017-11-04 07:12:27.332659 UTC] Starting iteration 80
[2017-11-04 07:12:27.332949 UTC] Start collecting samples
[2017-11-04 07:12:27.530536 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:27.548076 UTC] Performing policy update
[2017-11-04 07:12:27.548568 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:27.555970 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:27.645826 UTC] Performing line search
[2017-11-04 07:12:27.655263 UTC] Updating baseline
[2017-11-04 07:12:27.750420 UTC] Computing logging information
-------------------------------------
| Iteration            | 80         |
| ExpectedImprovement  | 0.016576   |
| ActualImprovement    | 0.013903   |
| ImprovementRatio     | 0.83877    |
| MeanKL               | 0.0065145  |
| Entropy              | 0.49239    |
| Perplexity           | 1.6362     |
| AveragePolicyProb[0] | 0.50225    |
| AveragePolicyProb[1] | 0.49775    |
| AverageReturn        | 194.86     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 20.657     |
| AverageEpisodeLength | 194.86     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 20.657     |
| TotalNEpisodes       | 1043       |
| TotalNSamples        | 1.6060e+05 |
| ExplainedVariance    | 0.28339    |
-------------------------------------
[2017-11-04 07:12:27.809578 UTC] Saving snapshot
[2017-11-04 07:12:27.814938 UTC] Starting iteration 81
[2017-11-04 07:12:27.815227 UTC] Start collecting samples
[2017-11-04 07:12:28.018442 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:28.035968 UTC] Performing policy update
[2017-11-04 07:12:28.036614 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:28.044241 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:28.132235 UTC] Performing line search
[2017-11-04 07:12:28.141305 UTC] Updating baseline
[2017-11-04 07:12:28.241796 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| ExpectedImprovement  | 0.013919   |
| ActualImprovement    | 0.011058   |
| ImprovementRatio     | 0.79448    |
| MeanKL               | 0.0064107  |
| Entropy              | 0.48201    |
| Perplexity           | 1.6193     |
| AveragePolicyProb[0] | 0.50469    |
| AveragePolicyProb[1] | 0.49531    |
| AverageReturn        | 194.07     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 22.335     |
| AverageEpisodeLength | 194.07     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 22.335     |
| TotalNEpisodes       | 1053       |
| TotalNSamples        | 1.6251e+05 |
| ExplainedVariance    | 0.33159    |
-------------------------------------
[2017-11-04 07:12:28.305633 UTC] Saving snapshot
[2017-11-04 07:12:28.311036 UTC] Starting iteration 82
[2017-11-04 07:12:28.311319 UTC] Start collecting samples
[2017-11-04 07:12:28.518435 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:28.534338 UTC] Performing policy update
[2017-11-04 07:12:28.534920 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:28.541867 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:28.631633 UTC] Performing line search
[2017-11-04 07:12:28.640398 UTC] Updating baseline
[2017-11-04 07:12:28.735599 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| ExpectedImprovement  | 0.013408   |
| ActualImprovement    | 0.013459   |
| ImprovementRatio     | 1.0038     |
| MeanKL               | 0.0066743  |
| Entropy              | 0.48959    |
| Perplexity           | 1.6316     |
| AveragePolicyProb[0] | 0.50854    |
| AveragePolicyProb[1] | 0.49146    |
| AverageReturn        | 193.19     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 23.772     |
| AverageEpisodeLength | 193.19     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.772     |
| TotalNEpisodes       | 1063       |
| TotalNSamples        | 1.6442e+05 |
| ExplainedVariance    | 0.24429    |
-------------------------------------
[2017-11-04 07:12:28.794523 UTC] Saving snapshot
[2017-11-04 07:12:28.799827 UTC] Starting iteration 83
[2017-11-04 07:12:28.800097 UTC] Start collecting samples
[2017-11-04 07:12:28.998978 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:29.015583 UTC] Performing policy update
[2017-11-04 07:12:29.016185 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:29.023653 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:29.112725 UTC] Performing line search
[2017-11-04 07:12:29.118545 UTC] Updating baseline
[2017-11-04 07:12:29.219055 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| ExpectedImprovement  | 0.014821   |
| ActualImprovement    | 0.0089386  |
| ImprovementRatio     | 0.60311    |
| MeanKL               | 0.0088059  |
| Entropy              | 0.50007    |
| Perplexity           | 1.6488     |
| AveragePolicyProb[0] | 0.50568    |
| AveragePolicyProb[1] | 0.49432    |
| AverageReturn        | 192.63     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 23.981     |
| AverageEpisodeLength | 192.63     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.981     |
| TotalNEpisodes       | 1074       |
| TotalNSamples        | 1.6656e+05 |
| ExplainedVariance    | 0.28074    |
-------------------------------------
[2017-11-04 07:12:29.277968 UTC] Saving snapshot
[2017-11-04 07:12:29.283340 UTC] Starting iteration 84
[2017-11-04 07:12:29.283619 UTC] Start collecting samples
[2017-11-04 07:12:29.489982 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:29.505499 UTC] Performing policy update
[2017-11-04 07:12:29.506053 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:29.513339 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:29.599475 UTC] Performing line search
[2017-11-04 07:12:29.604913 UTC] Updating baseline
[2017-11-04 07:12:29.706643 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| ExpectedImprovement  | 0.01506    |
| ActualImprovement    | 0.0087491  |
| ImprovementRatio     | 0.58093    |
| MeanKL               | 0.0094631  |
| Entropy              | 0.46799    |
| Perplexity           | 1.5968     |
| AveragePolicyProb[0] | 0.50323    |
| AveragePolicyProb[1] | 0.49677    |
| AverageReturn        | 192.56     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 24.033     |
| AverageEpisodeLength | 192.56     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 24.033     |
| TotalNEpisodes       | 1083       |
| TotalNSamples        | 1.6833e+05 |
| ExplainedVariance    | 0.2964     |
-------------------------------------
[2017-11-04 07:12:29.771981 UTC] Saving snapshot
[2017-11-04 07:12:29.777787 UTC] Starting iteration 85
[2017-11-04 07:12:29.778119 UTC] Start collecting samples
[2017-11-04 07:12:29.966524 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:29.982886 UTC] Performing policy update
[2017-11-04 07:12:29.983482 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:29.990771 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:30.076340 UTC] Performing line search
[2017-11-04 07:12:30.082792 UTC] Updating baseline
[2017-11-04 07:12:30.184200 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| ExpectedImprovement  | 0.018126   |
| ActualImprovement    | 0.01233    |
| ImprovementRatio     | 0.68025    |
| MeanKL               | 0.0083201  |
| Entropy              | 0.47732    |
| Perplexity           | 1.6118     |
| AveragePolicyProb[0] | 0.50407    |
| AveragePolicyProb[1] | 0.49593    |
| AverageReturn        | 192.12     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 24.26      |
| AverageEpisodeLength | 192.12     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 24.26      |
| TotalNEpisodes       | 1093       |
| TotalNSamples        | 1.7029e+05 |
| ExplainedVariance    | 0.14875    |
-------------------------------------
[2017-11-04 07:12:30.246509 UTC] Saving snapshot
[2017-11-04 07:12:30.251976 UTC] Starting iteration 86
[2017-11-04 07:12:30.252371 UTC] Start collecting samples
[2017-11-04 07:12:30.453129 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:30.469557 UTC] Performing policy update
[2017-11-04 07:12:30.470089 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:30.477440 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:30.555542 UTC] Performing line search
[2017-11-04 07:12:30.559930 UTC] Updating baseline
[2017-11-04 07:12:30.668842 UTC] Computing logging information
------------------------------------
| Iteration            | 86        |
| ExpectedImprovement  | 0.017517  |
| ActualImprovement    | 0.0078435 |
| ImprovementRatio     | 0.44775   |
| MeanKL               | 0.0070649 |
| Entropy              | 0.46728   |
| Perplexity           | 1.5957    |
| AveragePolicyProb[0] | 0.4989    |
| AveragePolicyProb[1] | 0.5011    |
| AverageReturn        | 191.3     |
| MinReturn            | 32        |
| MaxReturn            | 200       |
| StdReturn            | 24.678    |
| AverageEpisodeLength | 191.3     |
| MinEpisodeLength     | 32        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 24.678    |
| TotalNEpisodes       | 1104      |
| TotalNSamples        | 1.724e+05 |
| ExplainedVariance    | 0.20687   |
------------------------------------
[2017-11-04 07:12:30.723386 UTC] Saving snapshot
[2017-11-04 07:12:30.728509 UTC] Starting iteration 87
[2017-11-04 07:12:30.728719 UTC] Start collecting samples
[2017-11-04 07:12:30.942569 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:30.958871 UTC] Performing policy update
[2017-11-04 07:12:30.959336 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:30.966979 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:31.055643 UTC] Performing line search
[2017-11-04 07:12:31.065159 UTC] Updating baseline
[2017-11-04 07:12:31.169172 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| ExpectedImprovement  | 0.016311   |
| ActualImprovement    | 0.014985   |
| ImprovementRatio     | 0.91871    |
| MeanKL               | 0.0072173  |
| Entropy              | 0.4572     |
| Perplexity           | 1.5796     |
| AveragePolicyProb[0] | 0.51171    |
| AveragePolicyProb[1] | 0.48829    |
| AverageReturn        | 188.36     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 28.641     |
| AverageEpisodeLength | 188.36     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 28.641     |
| TotalNEpisodes       | 1115       |
| TotalNSamples        | 1.7431e+05 |
| ExplainedVariance    | 0.36461    |
-------------------------------------
[2017-11-04 07:12:31.228072 UTC] Saving snapshot
[2017-11-04 07:12:31.233333 UTC] Starting iteration 88
[2017-11-04 07:12:31.233550 UTC] Start collecting samples
[2017-11-04 07:12:31.440921 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:31.458718 UTC] Performing policy update
[2017-11-04 07:12:31.459336 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:31.467392 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:31.558544 UTC] Performing line search
[2017-11-04 07:12:31.564211 UTC] Updating baseline
[2017-11-04 07:12:31.729399 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| ExpectedImprovement  | 0.019825   |
| ActualImprovement    | 0.011757   |
| ImprovementRatio     | 0.59307    |
| MeanKL               | 0.008365   |
| Entropy              | 0.4415     |
| Perplexity           | 1.555      |
| AveragePolicyProb[0] | 0.51534    |
| AveragePolicyProb[1] | 0.48466    |
| AverageReturn        | 187.45     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 29.683     |
| AverageEpisodeLength | 187.45     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 29.683     |
| TotalNEpisodes       | 1127       |
| TotalNSamples        | 1.7662e+05 |
| ExplainedVariance    | 0.12919    |
-------------------------------------
[2017-11-04 07:12:31.805651 UTC] Saving snapshot
[2017-11-04 07:12:31.813332 UTC] Starting iteration 89
[2017-11-04 07:12:31.813613 UTC] Start collecting samples
[2017-11-04 07:12:32.031142 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:32.046123 UTC] Performing policy update
[2017-11-04 07:12:32.046578 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:32.053589 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:32.139441 UTC] Performing line search
[2017-11-04 07:12:32.144538 UTC] Updating baseline
[2017-11-04 07:12:32.251316 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| ExpectedImprovement  | 0.012973   |
| ActualImprovement    | 0.0094303  |
| ImprovementRatio     | 0.7269     |
| MeanKL               | 0.0087771  |
| Entropy              | 0.4382     |
| Perplexity           | 1.5499     |
| AveragePolicyProb[0] | 0.50156    |
| AveragePolicyProb[1] | 0.49844    |
| AverageReturn        | 189.61     |
| MinReturn            | 32         |
| MaxReturn            | 200        |
| StdReturn            | 28.395     |
| AverageEpisodeLength | 189.61     |
| MinEpisodeLength     | 32         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 28.395     |
| TotalNEpisodes       | 1136       |
| TotalNSamples        | 1.7842e+05 |
| ExplainedVariance    | 0.26176    |
-------------------------------------
[2017-11-04 07:12:32.316705 UTC] Saving snapshot
[2017-11-04 07:12:32.321859 UTC] Starting iteration 90
[2017-11-04 07:12:32.322099 UTC] Start collecting samples
[2017-11-04 07:12:32.527675 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:32.544269 UTC] Performing policy update
[2017-11-04 07:12:32.544740 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:32.551800 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:32.638286 UTC] Performing line search
[2017-11-04 07:12:32.643208 UTC] Updating baseline
[2017-11-04 07:12:32.741451 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| ExpectedImprovement  | 0.032472   |
| ActualImprovement    | 0.017749   |
| ImprovementRatio     | 0.54657    |
| MeanKL               | 0.0091048  |
| Entropy              | 0.45725    |
| Perplexity           | 1.5797     |
| AveragePolicyProb[0] | 0.51485    |
| AveragePolicyProb[1] | 0.48515    |
| AverageReturn        | 189.62     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 26.534     |
| AverageEpisodeLength | 189.62     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 26.534     |
| TotalNEpisodes       | 1149       |
| TotalNSamples        | 1.8067e+05 |
| ExplainedVariance    | 0.28653    |
-------------------------------------
[2017-11-04 07:12:32.804443 UTC] Saving snapshot
[2017-11-04 07:12:32.809550 UTC] Starting iteration 91
[2017-11-04 07:12:32.809768 UTC] Start collecting samples
[2017-11-04 07:12:32.999194 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:33.014793 UTC] Performing policy update
[2017-11-04 07:12:33.015258 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:33.022393 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:33.111215 UTC] Performing line search
[2017-11-04 07:12:33.119801 UTC] Updating baseline
[2017-11-04 07:12:33.214645 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| ExpectedImprovement  | 0.010472   |
| ActualImprovement    | 0.0072804  |
| ImprovementRatio     | 0.6952     |
| MeanKL               | 0.0070655  |
| Entropy              | 0.46371    |
| Perplexity           | 1.59       |
| AveragePolicyProb[0] | 0.49124    |
| AveragePolicyProb[1] | 0.50876    |
| AverageReturn        | 190.53     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 25.389     |
| AverageEpisodeLength | 190.53     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 25.389     |
| TotalNEpisodes       | 1159       |
| TotalNSamples        | 1.8267e+05 |
| ExplainedVariance    | 0.29856    |
-------------------------------------
[2017-11-04 07:12:33.278843 UTC] Saving snapshot
[2017-11-04 07:12:33.284994 UTC] Starting iteration 92
[2017-11-04 07:12:33.285228 UTC] Start collecting samples
[2017-11-04 07:12:33.484234 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:33.498411 UTC] Performing policy update
[2017-11-04 07:12:33.498877 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:33.505819 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:33.590083 UTC] Performing line search
[2017-11-04 07:12:33.594920 UTC] Updating baseline
[2017-11-04 07:12:33.697881 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| ExpectedImprovement  | 0.016032   |
| ActualImprovement    | 0.011846   |
| ImprovementRatio     | 0.73888    |
| MeanKL               | 0.007945   |
| Entropy              | 0.44254    |
| Perplexity           | 1.5567     |
| AveragePolicyProb[0] | 0.5014     |
| AveragePolicyProb[1] | 0.4986     |
| AverageReturn        | 190.53     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 25.389     |
| AverageEpisodeLength | 190.53     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 25.389     |
| TotalNEpisodes       | 1166       |
| TotalNSamples        | 1.8407e+05 |
| ExplainedVariance    | 0.20566    |
-------------------------------------
[2017-11-04 07:12:33.759732 UTC] Saving snapshot
[2017-11-04 07:12:33.765103 UTC] Starting iteration 93
[2017-11-04 07:12:33.765336 UTC] Start collecting samples
[2017-11-04 07:12:33.985780 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:34.011267 UTC] Performing policy update
[2017-11-04 07:12:34.011839 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:34.022702 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:34.153007 UTC] Performing line search
[2017-11-04 07:12:34.165965 UTC] Updating baseline
[2017-11-04 07:12:34.339984 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| ExpectedImprovement  | 0.011167   |
| ActualImprovement    | 0.0080948  |
| ImprovementRatio     | 0.72491    |
| MeanKL               | 0.0065515  |
| Entropy              | 0.45259    |
| Perplexity           | 1.5724     |
| AveragePolicyProb[0] | 0.52701    |
| AveragePolicyProb[1] | 0.47299    |
| AverageReturn        | 191.12     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 25.247     |
| AverageEpisodeLength | 191.12     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 25.247     |
| TotalNEpisodes       | 1177       |
| TotalNSamples        | 1.8627e+05 |
| ExplainedVariance    | 0.26265    |
-------------------------------------
[2017-11-04 07:12:34.432687 UTC] Saving snapshot
[2017-11-04 07:12:34.440765 UTC] Starting iteration 94
[2017-11-04 07:12:34.441121 UTC] Start collecting samples
[2017-11-04 07:12:34.677634 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:34.694514 UTC] Performing policy update
[2017-11-04 07:12:34.695582 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:34.702872 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:34.789015 UTC] Performing line search
[2017-11-04 07:12:34.798064 UTC] Updating baseline
[2017-11-04 07:12:34.905079 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| ExpectedImprovement  | 0.01199    |
| ActualImprovement    | 0.010848   |
| ImprovementRatio     | 0.90479    |
| MeanKL               | 0.0067689  |
| Entropy              | 0.42479    |
| Perplexity           | 1.5293     |
| AveragePolicyProb[0] | 0.50562    |
| AveragePolicyProb[1] | 0.49438    |
| AverageReturn        | 191.85     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 24.975     |
| AverageEpisodeLength | 191.85     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 24.975     |
| TotalNEpisodes       | 1189       |
| TotalNSamples        | 1.8867e+05 |
| ExplainedVariance    | 0.46162    |
-------------------------------------
[2017-11-04 07:12:34.965300 UTC] Saving snapshot
[2017-11-04 07:12:34.970344 UTC] Starting iteration 95
[2017-11-04 07:12:34.970584 UTC] Start collecting samples
[2017-11-04 07:12:35.178048 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:35.202200 UTC] Performing policy update
[2017-11-04 07:12:35.202894 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:35.213738 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:35.339937 UTC] Performing line search
[2017-11-04 07:12:35.352926 UTC] Updating baseline
[2017-11-04 07:12:35.499908 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| ExpectedImprovement  | 0.014594   |
| ActualImprovement    | 0.010005   |
| ImprovementRatio     | 0.68556    |
| MeanKL               | 0.0069251  |
| Entropy              | 0.43509    |
| Perplexity           | 1.5451     |
| AveragePolicyProb[0] | 0.48628    |
| AveragePolicyProb[1] | 0.51372    |
| AverageReturn        | 192.67     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 24.543     |
| AverageEpisodeLength | 192.67     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 24.543     |
| TotalNEpisodes       | 1197       |
| TotalNSamples        | 1.9027e+05 |
| ExplainedVariance    | 0.5168     |
-------------------------------------
[2017-11-04 07:12:35.564697 UTC] Saving snapshot
[2017-11-04 07:12:35.569820 UTC] Starting iteration 96
[2017-11-04 07:12:35.570028 UTC] Start collecting samples
[2017-11-04 07:12:35.759883 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:35.775898 UTC] Performing policy update
[2017-11-04 07:12:35.776510 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:35.784559 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:35.872996 UTC] Performing line search
[2017-11-04 07:12:35.883283 UTC] Updating baseline
[2017-11-04 07:12:36.007305 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| ExpectedImprovement  | 0.0082169  |
| ActualImprovement    | 0.006888   |
| ImprovementRatio     | 0.83827    |
| MeanKL               | 0.0065636  |
| Entropy              | 0.44171    |
| Perplexity           | 1.5554     |
| AveragePolicyProb[0] | 0.51691    |
| AveragePolicyProb[1] | 0.48309    |
| AverageReturn        | 193.98     |
| MinReturn            | 68         |
| MaxReturn            | 200        |
| StdReturn            | 23.079     |
| AverageEpisodeLength | 193.98     |
| MinEpisodeLength     | 68         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 23.079     |
| TotalNEpisodes       | 1208       |
| TotalNSamples        | 1.9247e+05 |
| ExplainedVariance    | 0.39783    |
-------------------------------------
[2017-11-04 07:12:36.073615 UTC] Saving snapshot
[2017-11-04 07:12:36.078887 UTC] Starting iteration 97
[2017-11-04 07:12:36.079158 UTC] Start collecting samples
[2017-11-04 07:12:36.296303 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:36.312489 UTC] Performing policy update
[2017-11-04 07:12:36.313001 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:36.320291 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:36.408452 UTC] Performing line search
[2017-11-04 07:12:36.414996 UTC] Updating baseline
[2017-11-04 07:12:36.512894 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| ExpectedImprovement  | 0.015669   |
| ActualImprovement    | 0.010827   |
| ImprovementRatio     | 0.691      |
| MeanKL               | 0.0095062  |
| Entropy              | 0.45504    |
| Perplexity           | 1.5762     |
| AveragePolicyProb[0] | 0.49694    |
| AveragePolicyProb[1] | 0.50306    |
| AverageReturn        | 195.61     |
| MinReturn            | 83         |
| MaxReturn            | 200        |
| StdReturn            | 19.124     |
| AverageEpisodeLength | 195.61     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 19.124     |
| TotalNEpisodes       | 1217       |
| TotalNSamples        | 1.9427e+05 |
| ExplainedVariance    | 0.47289    |
-------------------------------------
[2017-11-04 07:12:36.578978 UTC] Saving snapshot
[2017-11-04 07:12:36.584691 UTC] Starting iteration 98
[2017-11-04 07:12:36.584936 UTC] Start collecting samples
[2017-11-04 07:12:36.794995 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:36.811055 UTC] Performing policy update
[2017-11-04 07:12:36.811685 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:36.819315 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:36.898197 UTC] Performing line search
[2017-11-04 07:12:36.903062 UTC] Updating baseline
[2017-11-04 07:12:37.013728 UTC] Computing logging information
-------------------------------------
| Iteration            | 98         |
| ExpectedImprovement  | 0.016323   |
| ActualImprovement    | 0.013932   |
| ImprovementRatio     | 0.85355    |
| MeanKL               | 0.0080235  |
| Entropy              | 0.42809    |
| Perplexity           | 1.5343     |
| AveragePolicyProb[0] | 0.50546    |
| AveragePolicyProb[1] | 0.49454    |
| AverageReturn        | 196.52     |
| MinReturn            | 83         |
| MaxReturn            | 200        |
| StdReturn            | 17.032     |
| AverageEpisodeLength | 196.52     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 17.032     |
| TotalNEpisodes       | 1229       |
| TotalNSamples        | 1.9667e+05 |
| ExplainedVariance    | 0.51766    |
-------------------------------------
[2017-11-04 07:12:37.073045 UTC] Saving snapshot
[2017-11-04 07:12:37.078615 UTC] Starting iteration 99
[2017-11-04 07:12:37.078842 UTC] Start collecting samples
[2017-11-04 07:12:37.276487 UTC] Computing input variables for policy optimization
[2017-11-04 07:12:37.293695 UTC] Performing policy update
[2017-11-04 07:12:37.294325 UTC] Computing gradient in Euclidean space
[2017-11-04 07:12:37.302461 UTC] Computing approximate natural gradient using conjugate gradient algorithm
[2017-11-04 07:12:37.396188 UTC] Performing line search
[2017-11-04 07:12:37.401697 UTC] Updating baseline
[2017-11-04 07:12:37.506879 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| ExpectedImprovement  | 0.016176   |
| ActualImprovement    | 0.01329    |
| ImprovementRatio     | 0.82161    |
| MeanKL               | 0.0087119  |
| Entropy              | 0.42087    |
| Perplexity           | 1.5233     |
| AveragePolicyProb[0] | 0.50026    |
| AveragePolicyProb[1] | 0.49974    |
| AverageReturn        | 197.63     |
| MinReturn            | 83         |
| MaxReturn            | 200        |
| StdReturn            | 13.167     |
| AverageEpisodeLength | 197.63     |
| MinEpisodeLength     | 83         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 13.167     |
| TotalNEpisodes       | 1239       |
| TotalNSamples        | 1.9866e+05 |
| ExplainedVariance    | 0.59253    |
-------------------------------------
[2017-11-04 07:12:37.565973 UTC] Saving snapshot
